import numpy as np

def adaptiveLoss(lossList,weightList):
        diff=lossList[:-1]-lossList[:-2]
        square=np.squared(weightList[-1])
        unit=square/np.linalg.norm(square)
        if diff < 0:
                return -1*unit
        return unit

#if weights are improving from before, 
#generally we should increase bigger weights more and smaller weights less as it seems bigger weights more important
#square is to make sure its not linear and for other reasons later explained
#this makes it like you are speeding up or slowing down sine function fluctation used before
#square also makes speed up of indvidual factors more different, to explore different "offsets" or "phases" better
#does reverse direction if zero or negative, one 
#normalization keeps it bounded

