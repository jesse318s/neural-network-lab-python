{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Experiment Analysis Framework\n",
    "\n",
    "This notebook aggregates prior training artifacts from **neural-network-lab-python**, surfaces diagnostic visualizations, and recommends data-driven hyperparameter refinements for future experiments. It is designed to be reusable across training runs with minimal manual setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Validate the presence of required configs, logs, scalers, and weight checkpoints.\n",
    "2. Load active and historical configuration payloads and align them with training outcomes.\n",
    "3. Ingest `loss_history.csv`, `training_results.csv`, and particle simulation data for analytics.\n",
    "4. Reconstruct the latest model checkpoint, generate predictions, and evaluate residuals.\n",
    "5. Render visual diagnostics (loss curves, learning-rate sweeps, residual histograms, correlation heatmap).\n",
    "6. Summarize run health, recommend hyperparameter sweeps, and capture actionable next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from advanced_neural_network import AdvancedNeuralNetwork\n",
    "from data_processing import complete_data_pipeline, load_and_validate_data\n",
    "from ml_utils import compute_loss_weights\n",
    "from weight_constraints import BinaryWeightConstraintChanges, BinaryWeightConstraintMax, OscillationDampener\n",
    "\n",
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"neural-network-lab-python\"\n",
    "\n",
    "INPUT_FEATURES = [\n",
    "    \"mass\",\n",
    "    \"initial_velocity_x\",\n",
    "    \"initial_velocity_y\",\n",
    "    \"initial_position_x\",\n",
    "    \"initial_position_y\",\n",
    "    \"charge\",\n",
    "    \"magnetic_field_strength\",\n",
    "    \"simulation_time\"\n",
    "]\n",
    "\n",
    "OUTPUT_TARGETS = [\n",
    "    \"final_velocity_x\",\n",
    "    \"final_velocity_y\",\n",
    "    \"final_position_x\",\n",
    "    \"final_position_y\",\n",
    "    \"kinetic_energy\",\n",
    "    \"trajectory_length\"\n",
    "]\n",
    "\n",
    "ANALYSIS_SEED = 42\n",
    "\n",
    "np.random.seed(ANALYSIS_SEED)\n",
    "tf.random.set_seed(ANALYSIS_SEED)\n",
    "\n",
    "\n",
    "def format_bytes(size: Optional[int]) -> Optional[str]:\n",
    "    \"\"\"Format raw byte counts into human readable text.\"\"\"\n",
    "    if size is None: return None\n",
    "\n",
    "    threshold = 1024.0\n",
    "\n",
    "    units = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n",
    "\n",
    "    value = float(size)\n",
    "\n",
    "    for unit in units:\n",
    "        if value < threshold or unit == units[-1]: return f\"{value:.1f} {unit}\"\n",
    "\n",
    "        value /= threshold\n",
    "\n",
    "\n",
    "def resolve_project_paths() -> Dict[str, Path]:\n",
    "    \"\"\"Resolve key project directories relative to this notebook.\"\"\"\n",
    "    root = Path.cwd()\n",
    "\n",
    "    if root.name != PROJECT_NAME:\n",
    "        for parent in root.parents:\n",
    "            if parent.name == PROJECT_NAME: root = parent\n",
    "\n",
    "    config_dir = root / \"ml_config\"\n",
    "\n",
    "    output_dir = root / \"training_output\"\n",
    "\n",
    "    analysis_dir = output_dir / \"analysis\"\n",
    "\n",
    "    figures_dir = analysis_dir / \"figures\"\n",
    "\n",
    "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return {\n",
    "        \"project_root\": root,\n",
    "        \"config_dir\": config_dir,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"analysis_dir\": analysis_dir,\n",
    "        \"figures_dir\": figures_dir,\n",
    "        \"data_path\": root / \"particle_data.csv\",\n",
    "        \"scaler_X\": root / \"scaler_X.pkl\",\n",
    "        \"scaler_y\": root / \"scaler_y.pkl\",\n",
    "        \"weight_dir\": root / \"saved_weights/\"\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_required_artifacts(paths: Dict[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Check presence and metadata of required artifacts.\"\"\"\n",
    "    required = {\n",
    "        \"model_config\": paths[\"config_dir\"] / \"model_config.json\",\n",
    "        \"training_config\": paths[\"config_dir\"] / \"training_config.json\",\n",
    "        \"loss_history\": paths[\"output_dir\"] / \"loss_history.csv\",\n",
    "        \"training_results\": paths[\"output_dir\"] / \"training_results.csv\",\n",
    "        \"configuration_log\": paths[\"output_dir\"] / \"configuration_log.csv\",\n",
    "        \"particle_data\": paths[\"data_path\"],\n",
    "        \"scaler_X\": paths[\"scaler_X\"],\n",
    "        \"scaler_y\": paths[\"scaler_y\"]\n",
    "    }\n",
    "\n",
    "    optional = {\n",
    "        \"analysis_dir\": paths[\"analysis_dir\"],\n",
    "        \"figures_dir\": paths[\"figures_dir\"]\n",
    "    }\n",
    "\n",
    "    notes = {\n",
    "        \"particle_data\": \"Regenerate via data pipeline if missing.\",\n",
    "        \"scaler_X\": \"Rebuilt automatically through complete_data_pipeline.\",\n",
    "        \"scaler_y\": \"Rebuilt automatically through complete_data_pipeline.\"\n",
    "    }\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    def append_record(label: str, path: Path, critical: bool) -> None:\n",
    "        exists = path.exists()\n",
    "\n",
    "        size = path.stat().st_size if exists and path.is_file() else None\n",
    "\n",
    "        modified = pd.Timestamp(path.stat().st_mtime, unit=\"s\") if exists else None\n",
    "\n",
    "        records.append({\n",
    "            \"artifact\": label,\n",
    "            \"critical\": critical,\n",
    "            \"exists\": exists,\n",
    "            \"path\": str(path.relative_to(paths[\"project_root\"])) if exists else str(path),\n",
    "            \"size_bytes\": size,\n",
    "            \"size_readable\": format_bytes(size),\n",
    "            \"modified\": modified,\n",
    "            \"note\": notes.get(label)\n",
    "        })\n",
    "\n",
    "    for label, path in required.items():\n",
    "        append_record(label, path, True)\n",
    "\n",
    "    for label, path in optional.items():\n",
    "        append_record(label, path, False)\n",
    "\n",
    "    status_df = pd.DataFrame(records)\n",
    "\n",
    "    if status_df.empty: return status_df\n",
    "\n",
    "    status_df = status_df.sort_values([\"critical\", \"artifact\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    return status_df\n",
    "\n",
    "\n",
    "def list_checkpoint_weights(paths: Dict[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"List available weight checkpoints with epoch metadata.\"\"\"\n",
    "    pattern = \"model_weights_epoch_*.weights.h5\"\n",
    "\n",
    "    checkpoint_files = sorted(paths[\"weight_dir\"].glob(pattern))\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for file_path in checkpoint_files:\n",
    "        name = file_path.name\n",
    "\n",
    "        parts = name.split(\"_\")\n",
    "\n",
    "        epoch_token = parts[3] if len(parts) > 3 else parts[-1]\n",
    "\n",
    "        epoch = int(epoch_token.replace(\".weights.h5\", \"\")) if epoch_token else None\n",
    "\n",
    "        rows.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"name\": name,\n",
    "            \"path\": str(file_path.relative_to(paths[\"project_root\"])) if file_path.exists() else str(file_path),\n",
    "            \"modified\": pd.Timestamp(file_path.stat().st_mtime, unit=\"s\"),\n",
    "            \"size_bytes\": file_path.stat().st_size\n",
    "        })\n",
    "\n",
    "    checkpoint_df = pd.DataFrame(rows)\n",
    "\n",
    "    if checkpoint_df.empty: return checkpoint_df\n",
    "\n",
    "    checkpoint_df = checkpoint_df.sort_values(\"epoch\").reset_index(drop=True)\n",
    "\n",
    "    latest_epoch = checkpoint_df[\"epoch\"].max()\n",
    "\n",
    "    checkpoint_df[\"size_readable\"] = checkpoint_df[\"size_bytes\"].apply(format_bytes)\n",
    "\n",
    "    checkpoint_df[\"is_latest\"] = checkpoint_df[\"epoch\"] == latest_epoch\n",
    "\n",
    "    return checkpoint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(paths: Dict[str, Path]) -> Tuple[Dict[str, Any], Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"Load active configs and historical configuration snapshots with derived metrics.\"\"\"\n",
    "    model_config_path = paths[\"config_dir\"] / \"model_config.json\"\n",
    "\n",
    "    training_config_path = paths[\"config_dir\"] / \"training_config.json\"\n",
    "\n",
    "    with model_config_path.open() as handle:\n",
    "        model_config = json.load(handle)\n",
    "\n",
    "    with training_config_path.open() as handle:\n",
    "        training_config = json.load(handle)\n",
    "\n",
    "    snapshots: List[Dict[str, Any]] = []\n",
    "\n",
    "    for config_path in sorted(paths[\"output_dir\"].glob(\"training_config_*.json\")):\n",
    "        with config_path.open() as handle:\n",
    "            payload = json.load(handle)\n",
    "\n",
    "        combined: Dict[str, Any] = {\n",
    "            \"config_id\": payload.get(\"config_id\"),\n",
    "            \"timestamp\": payload.get(\"timestamp\")\n",
    "        }\n",
    "\n",
    "        model_payload = payload.get(\"model_config\", {})\n",
    "\n",
    "        for key, value in model_payload.items():\n",
    "            combined[key] = value\n",
    "\n",
    "        training_payload = payload.get(\"training_config\", {})\n",
    "\n",
    "        for key, value in training_payload.items():\n",
    "            combined[f\"train_{key}\"] = value\n",
    "\n",
    "        summary_payload = payload.get(\"performance_summary\", {})\n",
    "\n",
    "        combined[\"best_r2\"] = summary_payload.get(\"best_r2\")\n",
    "        combined[\"final_r2\"] = summary_payload.get(\"current_r2\")\n",
    "        combined[\"best_epoch\"] = summary_payload.get(\"best_r2_epoch\")\n",
    "        combined[\"avg_epoch_time_logged\"] = summary_payload.get(\"avg_epoch_time\")\n",
    "        combined[\"total_training_time\"] = summary_payload.get(\"total_training_time\")\n",
    "        combined[\"weight_modifications_used\"] = summary_payload.get(\"weight_modifications_used\")\n",
    "        combined[\"peak_memory_mb\"] = summary_payload.get(\"peak_memory_mb\")\n",
    "\n",
    "        snapshots.append(combined)\n",
    "\n",
    "    snapshots_df = pd.DataFrame(snapshots)\n",
    "\n",
    "    if snapshots_df.empty: return model_config, training_config, snapshots_df\n",
    "\n",
    "    snapshots_df[\"timestamp\"] = pd.to_datetime(snapshots_df[\"timestamp\"])\n",
    "\n",
    "    if {\"total_training_time\", \"train_epochs\"}.issubset(snapshots_df.columns):\n",
    "        snapshots_df[\"avg_epoch_time_calc\"] = snapshots_df[\"total_training_time\"] / snapshots_df[\"train_epochs\"]\n",
    "\n",
    "    snapshots_df[\"r2_delta\"] = snapshots_df[\"best_r2\"] - snapshots_df[\"final_r2\"]\n",
    "\n",
    "    snapshots_df = snapshots_df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    return model_config, training_config, snapshots_df\n",
    "\n",
    "\n",
    "def load_training_logs(paths: Dict[str, Path]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load loss history and training results with derived analytics.\"\"\"\n",
    "    loss_path = paths[\"output_dir\"] / \"loss_history.csv\"\n",
    "\n",
    "    results_path = paths[\"output_dir\"] / \"training_results.csv\"\n",
    "\n",
    "    loss_records = pd.read_csv(loss_path)\n",
    "\n",
    "    loss_records = loss_records.sort_values([\"epoch\"]).reset_index(drop=True)\n",
    "\n",
    "    loss_records[\"loss_ewm\"] = loss_records[\"combined_loss\"].ewm(alpha=0.15).mean()\n",
    "\n",
    "    epoch_summary = (\n",
    "        loss_records.groupby(\"epoch\").agg(\n",
    "            combined_loss_mean=(\"combined_loss\", \"mean\"),\n",
    "            combined_loss_std=(\"combined_loss\", \"std\"),\n",
    "            mae_mean=(\"mae\", \"mean\"),\n",
    "            mse_mean=(\"mse\", \"mean\")\n",
    "        ).reset_index()\n",
    "    )\n",
    "\n",
    "    results_df = pd.read_csv(results_path)\n",
    "\n",
    "    results_df[\"timestamp\"] = pd.to_datetime(results_df[\"timestamp\"])\n",
    "\n",
    "    results_df = results_df.sort_values(\"epoch\").reset_index(drop=True)\n",
    "\n",
    "    results_df[\"epoch\"] = results_df[\"epoch\"].astype(int)\n",
    "\n",
    "    results_df[\"cumulative_time\"] = results_df[\"epoch_time\"].cumsum()\n",
    "\n",
    "    results_df[\"val_loss_delta\"] = results_df[\"val_loss\"].diff()\n",
    "\n",
    "    results_df[\"train_val_gap\"] = results_df[\"val_loss\"] - results_df[\"train_loss\"]\n",
    "\n",
    "    results_df[\"val_mae_delta\"] = results_df[\"val_mae\"].diff()\n",
    "\n",
    "    results_df[\"epoch_time_rolling\"] = results_df[\"epoch_time\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "    results_df[\"memory_headroom_mb\"] = results_df[\"memory_mb\"].max() - results_df[\"memory_mb\"]\n",
    "\n",
    "    merged_metrics = results_df.merge(epoch_summary, on=\"epoch\", how=\"left\")\n",
    "\n",
    "    merged_metrics[\"val_loss_rolling\"] = merged_metrics[\"val_loss\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "    merged_metrics[\"train_loss_rolling\"] = merged_metrics[\"train_loss\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "    analytics = {\n",
    "        \"loss_records\": loss_records,\n",
    "        \"epoch_summary\": epoch_summary,\n",
    "        \"results\": results_df,\n",
    "        \"merged_metrics\": merged_metrics\n",
    "    }\n",
    "\n",
    "    return analytics\n",
    "\n",
    "\n",
    "def load_scalers(paths: Dict[str, Path]) -> Tuple[Any, Any]:\n",
    "    \"\"\"Load cached scalers, regenerating them via training pipeline if missing.\"\"\"\n",
    "    scaler_X_path = paths[\"scaler_X\"]\n",
    "\n",
    "    scaler_y_path = paths[\"scaler_y\"]\n",
    "\n",
    "    pipeline_ran = False\n",
    "\n",
    "    def ensure_pipeline() -> None:\n",
    "        nonlocal pipeline_ran\n",
    "\n",
    "        if pipeline_ran: return\n",
    "\n",
    "        complete_data_pipeline(csv_path=str(paths[\"data_path\"]))\n",
    "\n",
    "        pipeline_ran = True\n",
    "\n",
    "    try:\n",
    "        scaler_X = joblib.load(scaler_X_path)\n",
    "    except FileNotFoundError:\n",
    "        ensure_pipeline()\n",
    "\n",
    "        scaler_X = joblib.load(scaler_X_path)\n",
    "\n",
    "    try:\n",
    "        scaler_y = joblib.load(scaler_y_path)\n",
    "    except FileNotFoundError:\n",
    "        ensure_pipeline()\n",
    "\n",
    "        scaler_y = joblib.load(scaler_y_path)\n",
    "\n",
    "    return scaler_X, scaler_y\n",
    "\n",
    "\n",
    "def load_particle_data(paths: Dict[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Load particle simulation data with validation safeguards.\"\"\"\n",
    "    dataset = load_and_validate_data(csv_path=str(paths[\"data_path\"]))\n",
    "\n",
    "    if \"particle_id\" in dataset.columns:\n",
    "        dataset = dataset.sort_values(\"particle_id\").reset_index(drop=True)\n",
    "    else:\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_config(model_config: Dict[str, Any], training_config: Dict[str, Any]) -> tf.keras.Model:\n",
    "    \"\"\"Instantiate a compiled model that mirrors the training setup.\"\"\"\n",
    "    config_payload = dict(model_config)\n",
    "\n",
    "    config_payload.update(training_config)\n",
    "\n",
    "    config_payload.setdefault(\"enable_weight_oscillation_dampener\", True)\n",
    "\n",
    "    input_shape = (len(INPUT_FEATURES),)\n",
    "\n",
    "    output_shape = len(OUTPUT_TARGETS)\n",
    "\n",
    "    network = AdvancedNeuralNetwork(input_shape=input_shape, output_shape=output_shape, config=config_payload)\n",
    "\n",
    "    network.compile_model()\n",
    "\n",
    "    return network.model\n",
    "\n",
    "\n",
    "def load_model_checkpoint(paths: Dict[str, Path], model_config: Dict[str, Any], training_config: Dict[str, Any], checkpoint_index: pd.DataFrame, checkpoint_name: Optional[str] = None) -> Tuple[Optional[tf.keras.Model], Optional[Dict[str, Any]]]:\n",
    "    \"\"\"Load model weights from the selected checkpoint.\"\"\"\n",
    "    if checkpoint_index.empty: return None, None\n",
    "\n",
    "    if checkpoint_name is None:\n",
    "        selected_row = checkpoint_index.iloc[-1]\n",
    "    else:\n",
    "        if checkpoint_name not in checkpoint_index[\"name\"].values: return None, None\n",
    "\n",
    "        selected_row = checkpoint_index.loc[checkpoint_index[\"name\"] == checkpoint_name].iloc[0]\n",
    "\n",
    "    weights_path = paths[\"project_root\"] / selected_row[\"path\"]\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = build_model_from_config(model_config=model_config, training_config=training_config)\n",
    "\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    metadata = {\n",
    "        \"epoch\": int(selected_row[\"epoch\"]),\n",
    "        \"weights_path\": str(weights_path.relative_to(paths[\"project_root\"])),\n",
    "        \"size_bytes\": int(selected_row[\"size_bytes\"]),\n",
    "        \"size_readable\": selected_row.get(\"size_readable\"),\n",
    "        \"modified\": selected_row[\"modified\"],\n",
    "        \"parameter_count\": int(model.count_params())\n",
    "    }\n",
    "\n",
    "    return model, metadata\n",
    "\n",
    "\n",
    "def compute_predictions(model: Optional[tf.keras.Model], scaler_X: Any, scaler_y: Any, particle_df: pd.DataFrame, sample_size: int = 256) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Generate predictions and residual analytics using stored scalers.\"\"\"\n",
    "    if model is None: return pd.DataFrame(), {}\n",
    "\n",
    "    feature_subset = particle_df[INPUT_FEATURES].copy()\n",
    "\n",
    "    if sample_size and len(feature_subset) > sample_size:\n",
    "        feature_subset = feature_subset.sample(sample_size, random_state=ANALYSIS_SEED).sort_index()\n",
    "\n",
    "    scaled_inputs = scaler_X.transform(feature_subset.values) if scaler_X is not None else feature_subset.values\n",
    "\n",
    "    predictions_scaled = model.predict(scaled_inputs, verbose=0)\n",
    "\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled) if scaler_y is not None else predictions_scaled\n",
    "\n",
    "    actual_outputs = particle_df.loc[feature_subset.index, OUTPUT_TARGETS].values\n",
    "\n",
    "    residuals = predictions - actual_outputs\n",
    "\n",
    "    residual_df = pd.DataFrame(index=feature_subset.index)\n",
    "\n",
    "    if \"particle_id\" in particle_df.columns:\n",
    "        residual_df[\"particle_id\"] = particle_df.loc[feature_subset.index, \"particle_id\"]\n",
    "\n",
    "    for idx, target in enumerate(OUTPUT_TARGETS):\n",
    "        residual_df[f\"actual_{target}\"] = actual_outputs[:, idx]\n",
    "\n",
    "        residual_df[f\"pred_{target}\"] = predictions[:, idx]\n",
    "\n",
    "        residual_df[f\"residual_{target}\"] = residuals[:, idx]\n",
    "\n",
    "    residual_df[\"residual_norm\"] = np.linalg.norm(residuals, axis=1)\n",
    "\n",
    "    residual_norm_mean = residual_df[\"residual_norm\"].mean()\n",
    "\n",
    "    residual_norm_std = residual_df[\"residual_norm\"].std(ddof=0)\n",
    "\n",
    "    if residual_norm_std and residual_norm_std > 0:\n",
    "        residual_df[\"residual_norm_z\"] = (residual_df[\"residual_norm\"] - residual_norm_mean) / residual_norm_std\n",
    "\n",
    "    mae_value = float(np.mean(np.abs(residuals)))\n",
    "\n",
    "    rmse_value = float(np.sqrt(np.mean(np.square(residuals))))\n",
    "\n",
    "    target_metrics: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for idx, target in enumerate(OUTPUT_TARGETS):\n",
    "        target_residuals = residuals[:, idx]\n",
    "\n",
    "        target_metrics[target] = {\n",
    "            \"mae\": float(np.mean(np.abs(target_residuals))),\n",
    "            \"rmse\": float(np.sqrt(np.mean(np.square(target_residuals)))),\n",
    "            \"bias\": float(np.mean(target_residuals))\n",
    "        }\n",
    "\n",
    "    metrics: Dict[str, Any] = {\n",
    "        \"samples\": int(len(residual_df)),\n",
    "        \"mae\": mae_value,\n",
    "        \"rmse\": rmse_value,\n",
    "        \"residual_norm_median\": float(residual_df[\"residual_norm\"].median()),\n",
    "        \"residual_norm_p95\": float(residual_df[\"residual_norm\"].quantile(0.95)),\n",
    "        \"targets\": target_metrics\n",
    "    }\n",
    "\n",
    "    return residual_df, metrics\n",
    "\n",
    "\n",
    "def summarize_run_performance(results_df: pd.DataFrame, epoch_summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a concise summary of key performance indicators.\"\"\"\n",
    "    if results_df.empty: return pd.DataFrame()\n",
    "\n",
    "    best_epoch_idx = int(results_df[\"val_loss\"].idxmin())\n",
    "\n",
    "    best_row = results_df.loc[best_epoch_idx]\n",
    "\n",
    "    final_row = results_df.iloc[-1]\n",
    "\n",
    "    early_row = results_df.iloc[0]\n",
    "\n",
    "    improvement = float(early_row[\"val_loss\"] - best_row[\"val_loss\"])\n",
    "\n",
    "    consistency = float(epoch_summary[\"combined_loss_std\"].tail(5).mean()) if not epoch_summary.empty else float(\"nan\")\n",
    "\n",
    "    best_r2_row = results_df.loc[results_df[\"r2_score\"].idxmax()]\n",
    "\n",
    "    summary = pd.DataFrame([\n",
    "        {\"metric\": \"Best validation loss\", \"value\": best_row[\"val_loss\"], \"notes\": f\"Epoch {int(best_row['epoch'])}\"},\n",
    "        {\"metric\": \"Final validation loss\", \"value\": final_row[\"val_loss\"], \"notes\": f\"Train gap {final_row['train_val_gap']:.4f}\"},\n",
    "        {\"metric\": \"Validation improvement\", \"value\": improvement, \"notes\": \"Drop from first to best epoch\"},\n",
    "        {\"metric\": \"Validation stability (std last 5 epochs)\", \"value\": consistency, \"notes\": \"Lower is more stable\"},\n",
    "        {\"metric\": \"Average epoch time (last 10 epochs)\", \"value\": results_df[\"epoch_time\"].tail(10).mean(), \"notes\": \"Supports batch-size experiments\"},\n",
    "        {\"metric\": \"Peak R²\", \"value\": best_r2_row[\"r2_score\"], \"notes\": f\"Epoch {int(best_r2_row['epoch'])}\"},\n",
    "        {\"metric\": \"Total recorded training time\", \"value\": results_df[\"epoch_time\"].sum(), \"notes\": \"seconds\"}\n",
    "    ])\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def suggest_hyperparameters(model_config: Dict[str, Any], training_config: Dict[str, Any], config_history: pd.DataFrame, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Derive hyperparameter sweep recommendations from observed metrics.\"\"\"\n",
    "    if results_df.empty: return pd.DataFrame()\n",
    "\n",
    "    suggestions: List[Dict[str, Any]] = []\n",
    "\n",
    "    base_lr = float(model_config.get(\"learning_rate\", 0.001))\n",
    "\n",
    "    final_window = results_df.tail(5)\n",
    "\n",
    "    val_loss_range = float(final_window[\"val_loss\"].max() - final_window[\"val_loss\"].min())\n",
    "\n",
    "    best_epoch = int(results_df.loc[results_df[\"val_loss\"].idxmin(), \"epoch\"])\n",
    "\n",
    "    final_epoch = int(results_df.iloc[-1][\"epoch\"])\n",
    "\n",
    "    total_epochs = int(training_config.get(\"epochs\", final_epoch + 1))\n",
    "\n",
    "    if val_loss_range < 0.01 and final_epoch - best_epoch > 5:\n",
    "        proposals = sorted({round(base_lr * factor, 6) for factor in (0.5, 0.8, 1.2)})\n",
    "\n",
    "        suggestions.append({\n",
    "            \"parameter\": \"learning_rate\",\n",
    "            \"proposed_values\": proposals,\n",
    "            \"rationale\": \"Validation loss plateaued across the last epochs; nudge the optimizer step to reintroduce progress.\",\n",
    "            \"constraints\": \"Keep BinaryWeightConstraintMax(max_binary_digits=5) engaged for stability.\"\n",
    "        })\n",
    "\n",
    "    train_val_gap = float(final_window[\"train_val_gap\"].mean())\n",
    "\n",
    "    if train_val_gap > 0.05:\n",
    "        suggestions.append({\n",
    "            \"parameter\": \"dropout_rate\",\n",
    "            \"proposed_values\": [0.05, 0.1, 0.15],\n",
    "            \"rationale\": \"Consistent validation > training loss points to mild overfitting; mild dropout can regularize activations.\",\n",
    "            \"constraints\": \"Retain enable_weight_oscillation_dampener=True to temper weight swings.\"\n",
    "        })\n",
    "\n",
    "    avg_epoch_time = float(results_df[\"epoch_time\"].tail(10).mean())\n",
    "\n",
    "    memory_headroom = float(results_df[\"memory_headroom_mb\"].tail(10).mean())\n",
    "\n",
    "    if avg_epoch_time < 1.5 and memory_headroom > 0:\n",
    "        baseline_batch = int(training_config.get(\"batch_size\", 16))\n",
    "\n",
    "        candidate_batches = sorted({baseline_batch, 24, 32})\n",
    "\n",
    "        suggestions.append({\n",
    "            \"parameter\": \"batch_size\",\n",
    "            \"proposed_values\": candidate_batches,\n",
    "            \"rationale\": \"Epoch time and memory logs show headroom; larger batches could reduce gradient variance.\",\n",
    "            \"constraints\": \"Validate GPU memory against peak usage before committing.\"\n",
    "        })\n",
    "\n",
    "    if final_epoch >= total_epochs - 2:\n",
    "        extension_epochs = sorted({total_epochs + 10, total_epochs + 20})\n",
    "\n",
    "        suggestions.append({\n",
    "            \"parameter\": \"epochs\",\n",
    "            \"proposed_values\": extension_epochs,\n",
    "            \"rationale\": \"Best epoch occurs near training ceiling; extending training may unlock additional gains.\",\n",
    "            \"constraints\": \"Monitor for overfitting; stop early if val loss degrades.\"\n",
    "        })\n",
    "\n",
    "    if not config_history.empty and \"learning_rate\" in config_history.columns:\n",
    "        grouped = config_history.groupby(\"learning_rate\")[\"final_r2\"].mean().sort_values()\n",
    "\n",
    "        if len(grouped) > 1:\n",
    "            top_lr = grouped.idxmax()\n",
    "\n",
    "            if abs(top_lr - base_lr) / base_lr > 0.2:\n",
    "                suggestions.append({\n",
    "                    \"parameter\": \"learning_rate\",\n",
    "                    \"proposed_values\": [round(float(top_lr), 6)],\n",
    "                    \"rationale\": \"Historical sweep points to a different learning rate yielding higher final R².\",\n",
    "                    \"constraints\": \"Pair with BinaryWeightConstraintChanges() to keep update granularity consistent.\"\n",
    "                })\n",
    "\n",
    "    if suggestions:\n",
    "        recommendations = pd.DataFrame(suggestions)\n",
    "\n",
    "        return recommendations.drop_duplicates(subset=[\"parameter\", \"rationale\"])\n",
    "\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = resolve_project_paths()\n",
    "\n",
    "display(Markdown(f\"**Project root:** `{paths['project_root']}`\"))\n",
    "\n",
    "artifact_status = validate_required_artifacts(paths)\n",
    "\n",
    "display(Markdown(\"### Artifact Inventory\"))\n",
    "\n",
    "display(artifact_status)\n",
    "\n",
    "missing_artifacts = artifact_status.loc[~artifact_status[\"exists\"]]\n",
    "\n",
    "if not missing_artifacts.empty:\n",
    "    display(Markdown(\"⚠️ **Missing artifacts detected. Review notes before continuing.**\"))\n",
    "\n",
    "    display(missing_artifacts)\n",
    "else:\n",
    "    display(Markdown(\"✅ All critical artifacts are present.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config, training_config, config_history = load_configs(paths)\n",
    "\n",
    "display(Markdown(\"### Active Model Configuration\"))\n",
    "\n",
    "display(pd.Series(model_config, name=\"model_config\"))\n",
    "\n",
    "display(Markdown(\"### Active Training Configuration\"))\n",
    "\n",
    "display(pd.Series(training_config, name=\"training_config\"))\n",
    "\n",
    "if not config_history.empty:\n",
    "    display(Markdown(\"### Historical Configuration Snapshots\"))\n",
    "\n",
    "    history_columns = [\n",
    "        col\n",
    "        for col in [\n",
    "            \"timestamp\", \"config_id\", \"learning_rate\", \"dropout_rate\", \"train_batch_size\", \"train_epochs\", \"best_r2\", \"final_r2\", \"r2_delta\", \"avg_epoch_time_logged\", \"avg_epoch_time_calc\", \"total_training_time\"\n",
    "        ]\n",
    "        if col in config_history.columns\n",
    "    ]\n",
    "\n",
    "    display(config_history[history_columns])\n",
    "\n",
    "    numeric_cols = [col for col in history_columns if config_history[col].dtype.kind in \"if\"]\n",
    "\n",
    "    if numeric_cols:\n",
    "        history_stats = config_history[numeric_cols].describe().transpose()\n",
    "\n",
    "        display(Markdown(\"#### Configuration Summary Statistics\"))\n",
    "\n",
    "        display(history_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = load_training_logs(paths)\n",
    "\n",
    "loss_records = analytics[\"loss_records\"]\n",
    "\n",
    "epoch_summary = analytics[\"epoch_summary\"]\n",
    "\n",
    "results_df = analytics[\"results\"]\n",
    "\n",
    "merged_metrics = analytics[\"merged_metrics\"]\n",
    "\n",
    "display(Markdown(\"### Epoch-Level Performance Summary\"))\n",
    "\n",
    "display(results_df.tail(10)[[\"epoch\", \"train_loss\", \"val_loss\", \"train_val_gap\", \"val_loss_delta\", \"epoch_time\"]])\n",
    "\n",
    "performance_snapshot = summarize_run_performance(results_df, epoch_summary)\n",
    "\n",
    "display(Markdown(\"### Key Performance Indicators\"))\n",
    "\n",
    "display(performance_snapshot)\n",
    "\n",
    "display(Markdown(\"#### Loss Distribution by Epoch\"))\n",
    "\n",
    "display(epoch_summary.tail(10))\n",
    "\n",
    "display(Markdown(\"#### Exponential Moving Average of Combined Loss\"))\n",
    "\n",
    "display(loss_records.tail(10)[[\"epoch\", \"combined_loss\", \"loss_ewm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_df = load_particle_data(paths)\n",
    "\n",
    "scaler_X, scaler_y = load_scalers(paths)\n",
    "\n",
    "display(Markdown(\"### Particle Data Snapshot\"))\n",
    "\n",
    "display(Markdown(f\"Dataset shape: **{particle_df.shape[0]}** rows × **{particle_df.shape[1]}** columns\"))\n",
    "\n",
    "display(particle_df.head())\n",
    "\n",
    "display(Markdown(\"#### Descriptive Statistics\"))\n",
    "\n",
    "display(particle_df.describe(include=\"all\").transpose())\n",
    "\n",
    "missing_counts = particle_df.isna().sum()\n",
    "\n",
    "if missing_counts.any():\n",
    "    display(Markdown(\"#### Missing Value Audit\"))\n",
    "\n",
    "    display(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_index = list_checkpoint_weights(paths)\n",
    "\n",
    "display(Markdown(\"### Available Weight Checkpoints\"))\n",
    "\n",
    "if checkpoint_index.empty:\n",
    "    display(Markdown(\"No checkpoints found. Run training to generate weight artifacts.\"))\n",
    "else:\n",
    "    display(checkpoint_index)\n",
    "\n",
    "model, checkpoint_meta = load_model_checkpoint(paths, model_config, training_config, checkpoint_index)\n",
    "\n",
    "if checkpoint_meta is not None:\n",
    "    display(Markdown(f\"Loaded checkpoint: **epoch {checkpoint_meta['epoch']}** from `{checkpoint_meta['weights_path']}`\"))\n",
    "\n",
    "    display(pd.Series(checkpoint_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_df, residual_metrics = compute_predictions(model, scaler_X, scaler_y, particle_df)\n",
    "\n",
    "if residual_metrics:\n",
    "    display(Markdown(\"### Residual Metrics\"))\n",
    "\n",
    "    overall_metrics = {key: value for key, value in residual_metrics.items() if key != \"targets\"}\n",
    "\n",
    "    display(pd.Series(overall_metrics, name=\"residual_metrics\"))\n",
    "\n",
    "    target_metrics = pd.DataFrame(residual_metrics[\"targets\"]).transpose()\n",
    "\n",
    "    display(Markdown(\"#### Per-Target Residual Summary\"))\n",
    "\n",
    "    display(target_metrics)\n",
    "\n",
    "if not residuals_df.empty:\n",
    "    display(Markdown(\"### Residual Sample\"))\n",
    "\n",
    "    display(residuals_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = paths[\"figures_dir\"]\n",
    "\n",
    "# Loss trend\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.lineplot(data=results_df, x=\"epoch\", y=\"train_loss\", ax=ax, label=\"Train Loss\")\n",
    "sns.lineplot(data=results_df, x=\"epoch\", y=\"val_loss\", ax=ax, label=\"Validation Loss\")\n",
    "val_std = results_df[\"val_loss\"].rolling(5, min_periods=1).std()\n",
    "ax.fill_between(results_df[\"epoch\"], results_df[\"val_loss\"] - val_std, results_df[\"val_loss\"] + val_std, color=\"tab:blue\", alpha=0.1)\n",
    "ax.set_title(\"Training vs Validation Loss\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "fig.tight_layout()\n",
    "loss_curve_path = figures_dir / \"loss_curves.png\"\n",
    "fig.savefig(loss_curve_path, dpi=200)\n",
    "plt.close(fig)\n",
    "display(Markdown(f\"Saved loss curves to `{loss_curve_path}`\"))\n",
    "\n",
    "# Train vs validation gap\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.lineplot(data=results_df, x=\"epoch\", y=\"train_val_gap\", ax=ax, color=\"tab:red\")\n",
    "ax.axhline(0, linestyle=\"--\", color=\"grey\", linewidth=1)\n",
    "ax.set_title(\"Train vs Validation Gap\")\n",
    "ax.set_ylabel(\"Val - Train Loss\")\n",
    "fig.tight_layout()\n",
    "gap_plot_path = figures_dir / \"train_val_gap.png\"\n",
    "fig.savefig(gap_plot_path, dpi=200)\n",
    "plt.close(fig)\n",
    "display(Markdown(f\"Saved train/val gap chart to `{gap_plot_path}`\"))\n",
    "\n",
    "# Learning rate vs final loss metrics\n",
    "if not config_history.empty:\n",
    "    lr_df = config_history.copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(data=lr_df, x=\"learning_rate\", y=\"final_r2\", size=\"total_training_time\", hue=\"final_r2\", palette=\"viridis\", ax=ax)\n",
    "    ax.set_title(\"Learning Rate vs Final R²\")\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Final R²\")\n",
    "    fig.tight_layout()\n",
    "    lr_plot_path = figures_dir / \"learning_rate_vs_r2.png\"\n",
    "    fig.savefig(lr_plot_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    display(Markdown(f\"Saved learning-rate diagnostics to `{lr_plot_path}`\"))\n",
    "\n",
    "# Residual histogram\n",
    "if not residuals_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals_df[\"residual_norm\"], bins=30, ax=ax, kde=True, color=\"tab:orange\")\n",
    "    ax.set_title(\"Residual Norm Distribution\")\n",
    "    ax.set_xlabel(\"Residual Norm\")\n",
    "    fig.tight_layout()\n",
    "    residual_hist_path = figures_dir / \"residual_norm_hist.png\"\n",
    "    fig.savefig(residual_hist_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    display(Markdown(f\"Saved residual histogram to `{residual_hist_path}`\"))\n",
    "\n",
    "    residual_columns = [col for col in residuals_df.columns if col.startswith(\"residual_\") and any(col.endswith(target) for target in OUTPUT_TARGETS)]\n",
    "\n",
    "    if residual_columns:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        melted = residuals_df[residual_columns].melt(var_name=\"target\", value_name=\"residual\")\n",
    "        sns.boxplot(data=melted, x=\"target\", y=\"residual\", ax=ax)\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        ax.set_title(\"Residual Distribution by Target\")\n",
    "        fig.tight_layout()\n",
    "        residual_box_path = figures_dir / \"residual_distribution_by_target.png\"\n",
    "        fig.savefig(residual_box_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        display(Markdown(f\"Saved residual distribution boxplot to `{residual_box_path}`\"))\n",
    "\n",
    "# Correlation heatmap\n",
    "heatmap_features = [\"train_loss\", \"val_loss\", \"train_mae\", \"val_mae\", \"r2_score\", \"epoch_time\", \"train_val_gap\", \"memory_headroom_mb\"]\n",
    "usable_cols = [col for col in heatmap_features if col in merged_metrics.columns]\n",
    "\n",
    "if usable_cols:\n",
    "    corr_matrix = merged_metrics[usable_cols].corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "    ax.set_title(\"Metric Correlation Heatmap\")\n",
    "    fig.tight_layout()\n",
    "    heatmap_path = figures_dir / \"metric_correlation_heatmap.png\"\n",
    "    fig.savefig(heatmap_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    display(Markdown(f\"Saved correlation heatmap to `{heatmap_path}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = suggest_hyperparameters(model_config, training_config, config_history, results_df)\n",
    "\n",
    "if not recommendations_df.empty:\n",
    "    display(Markdown(\"### Recommended Hyperparameter Sweeps\"))\n",
    "\n",
    "    display(recommendations_df)\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"No immediate hyperparameter adjustments detected beyond current configuration.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_items: List[str] = []\n",
    "\n",
    "if not results_df.empty:\n",
    "    final_row = results_df.iloc[-1]\n",
    "\n",
    "    best_row = results_df.loc[results_df[\"val_loss\"].idxmin()]\n",
    "\n",
    "    insight_items.append(f\"Best validation loss {best_row['val_loss']:.4f} at epoch {int(best_row['epoch'])}.\")\n",
    "\n",
    "    insight_items.append(f\"Validation plateau range over last window: {(results_df.tail(5)['val_loss'].max() - results_df.tail(5)['val_loss'].min()):.4f}.\")\n",
    "\n",
    "    insight_items.append(f\"Train/val gap at final epoch: {final_row['train_val_gap']:.4f}.\")\n",
    "\n",
    "    insight_items.append(f\"Cumulative training time logged: {results_df['epoch_time'].sum():.1f} seconds.\")\n",
    "\n",
    "if residual_metrics:\n",
    "    insight_items.append(f\"Mean absolute residual across sampled predictions: {residual_metrics['mae']:.4f}.\")\n",
    "\n",
    "    insight_items.append(f\"95th percentile residual norm: {residual_metrics['residual_norm_p95']:.4f}.\")\n",
    "\n",
    "if not recommendations_df.empty:\n",
    "    suggested = \", \".join(recommendations_df[\"parameter\"].unique())\n",
    "\n",
    "    insight_items.append(f\"Hyperparameter sweep targets: {suggested}.\")\n",
    "\n",
    "missing_artifacts = artifact_status.loc[~artifact_status[\"exists\"] & artifact_status[\"critical\"]]\n",
    "\n",
    "if not missing_artifacts.empty:\n",
    "    missing_list = \", \".join(missing_artifacts[\"artifact\"].tolist())\n",
    "\n",
    "    insight_items.append(f\"Critical artifacts missing: {missing_list}.\")\n",
    "\n",
    "if not insight_items:\n",
    "    insight_items.append(\"Insufficient data to derive insights.\")\n",
    "\n",
    "display(Markdown(\"### Insight Summary\"))\n",
    "\n",
    "for item in insight_items:\n",
    "    display(Markdown(f\"- {item}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_notebook_smoke_test() -> Dict[str, Any]:\n",
    "    \"\"\"Validate that core notebook stages complete without exceptions.\"\"\"\n",
    "    status = {\n",
    "        \"critical_artifacts_present\": bool(artifact_status.loc[artifact_status[\"critical\"] & ~artifact_status[\"exists\"]].empty),\n",
    "        \"config_history_entries\": int(len(config_history)),\n",
    "        \"loss_records\": int(len(loss_records)),\n",
    "        \"results_records\": int(len(results_df)),\n",
    "        \"residual_samples\": int(len(residuals_df)),\n",
    "        \"recommendations\": int(len(recommendations_df)),\n",
    "        \"figures_exported\": len(list(paths[\"figures_dir\"].glob(\"*.png\"))),\n",
    "        \"latest_checkpoint_epoch\": int(checkpoint_meta[\"epoch\"]) if checkpoint_meta else None\n",
    "    }\n",
    "\n",
    "    return status\n",
    "\n",
    "\n",
    "smoke_test_status = run_notebook_smoke_test()\n",
    "\n",
    "display(Markdown(\"### Validation Checklist\"))\n",
    "\n",
    "display(pd.Series(smoke_test_status, name=\"notebook_validation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Actionable Next Steps\n",
    "\n",
    "- Re-run the training pipeline after trialing the proposed learning-rate, dropout, and batch-size combinations; capture new config snapshots for comparison.\n",
    "- Promote saved figures under `training_output/analysis/figures/` into experiment reports or dashboards.\n",
    "- Extend this notebook with automated sweeps (GridSearch or Bayesian optimization) once additional configuration diversity is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Reuse Tips\n",
    "\n",
    "- Parameterize `sample_size` within `compute_predictions` to scale residual analysis for larger datasets.\n",
    "- Import this notebook’s helper functions via `%run experiment_analysis_framework.ipynb` inside future analysis notebooks for rapid setup.\n",
    "- Store additional diagnostics (e.g., feature importance, SHAP values) within the `analysis` directory for cross-experiment benchmarking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
