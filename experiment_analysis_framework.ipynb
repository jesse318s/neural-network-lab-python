{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fc8b73",
   "metadata": {},
   "source": [
    "# Experiment Analysis Framework\n",
    "\n",
    "**Purpose:** Comprehensive analysis tool for neural network training experiments with focus on data visualization, statistical insights, and actionable recommendations.\n",
    "\n",
    "**Key Features:**\n",
    "- Training dynamics visualization with interactive dashboards\n",
    "- Statistical hyperparameter recommendations with confidence intervals\n",
    "- Comprehensive benchmarking against baseline models\n",
    "- James-Stein estimator comparison for weight constraint evaluation\n",
    "- Residual analysis and error distribution diagnostics\n",
    "- Cross-run comparison and historical pattern recognition\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells sequentially to load artifacts and generate visualizations\n",
    "2. Review the comprehensive metric dashboards for training insights\n",
    "3. Examine hyperparameter recommendations for next experiments\n",
    "4. Compare binary weight constraints against James-Stein shrinkage\n",
    "\n",
    "**Dependencies:** TensorFlow, NumPy, Pandas, Matplotlib, Seaborn, SciPy, Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b503d7",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5739b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment Analysis Framework - Configuration and Imports\n",
    "Comprehensive analysis of neural network training experiments with statistical rigor.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import Markdown, display\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Reload the utils module to get latest changes\n",
    "import importlib\n",
    "import experiment_analysis_utils as utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_SEED = 42\n",
    "SAMPLE_SIZE = 256\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(ANALYSIS_SEED)\n",
    "tf.random.set_seed(ANALYSIS_SEED)\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"✓ Analysis framework initialized successfully\")\n",
    "print(f\"Random seed: {ANALYSIS_SEED}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0cdf1",
   "metadata": {},
   "source": [
    "### Project Path Detection and Artifact Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d51d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve project paths\n",
    "paths = utils.resolve_project_paths()\n",
    "\n",
    "display(Markdown(f\"**Project root:** `{paths['project_root']}`\"))\n",
    "display(Markdown(f\"**Figures directory:** `{paths['figures_dir']}`\"))\n",
    "\n",
    "# Validate artifacts\n",
    "artifact_status = utils.validate_required_artifacts(paths)\n",
    "\n",
    "display(Markdown(\"### Artifact Inventory\"))\n",
    "display(artifact_status)\n",
    "\n",
    "# Check for missing critical artifacts\n",
    "missing_artifacts = artifact_status.loc[~artifact_status[\"exists\"] & artifact_status[\"critical\"]]\n",
    "\n",
    "if not missing_artifacts.empty:\n",
    "    display(Markdown(\"⚠️ **Missing critical artifacts detected. Review notes before continuing.**\"))\n",
    "    display(missing_artifacts)\n",
    "else:\n",
    "    display(Markdown(\"✅ All critical artifacts are present.\"))\n",
    "    print(f\"\\nTotal artifacts validated: {len(artifact_status)}\")\n",
    "    print(f\"Critical artifacts: {artifact_status['critical'].sum()}\")\n",
    "    print(f\"Optional artifacts: {(~artifact_status['critical']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a8f62",
   "metadata": {},
   "source": [
    "## 2. Data Loading Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce064f3",
   "metadata": {},
   "source": [
    "### Load All Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "model_config, training_config, config_history = utils.load_configs(paths)\n",
    "\n",
    "# Load training logs\n",
    "analytics = utils.load_training_logs(paths)\n",
    "loss_records = analytics[\"loss_records\"]\n",
    "epoch_summary = analytics[\"epoch_summary\"]\n",
    "results_df = analytics[\"results\"]\n",
    "merged_metrics = analytics[\"merged_metrics\"]\n",
    "\n",
    "# Load particle data and scalers\n",
    "particle_df = utils.load_particle_data(paths)\n",
    "scaler_X, scaler_y = utils.load_scalers(paths)\n",
    "\n",
    "# Validate scaler compatibility with current data\n",
    "expected_features = len(utils.INPUT_FEATURES)\n",
    "expected_targets = len(utils.OUTPUT_TARGETS)\n",
    "\n",
    "scaler_features_mismatch = False\n",
    "if hasattr(scaler_X, 'n_features_in_') and scaler_X.n_features_in_ != expected_features:\n",
    "    print(f\"⚠️ Warning: Scaler expects {scaler_X.n_features_in_} input features but data has {expected_features}\")\n",
    "    print(\"This indicates the feature set has changed since scalers were saved.\")\n",
    "    scaler_features_mismatch = True\n",
    "\n",
    "if hasattr(scaler_y, 'n_features_in_') and scaler_y.n_features_in_ != expected_targets:\n",
    "    print(f\"⚠️ Warning: Scaler expects {scaler_y.n_features_in_} output targets but data has {expected_targets}\")\n",
    "    print(\"This indicates the target set has changed since scalers were saved.\")\n",
    "    scaler_features_mismatch = True\n",
    "\n",
    "if scaler_features_mismatch:\n",
    "    print(\"Scalers will be regenerated when needed for compatibility.\\n\")\n",
    "\n",
    "# Load model checkpoint with error handling\n",
    "checkpoint_index = utils.list_checkpoint_weights(paths)\n",
    "model = None\n",
    "checkpoint_meta = None\n",
    "\n",
    "try:\n",
    "    model, checkpoint_meta = utils.load_model_checkpoint(\n",
    "        paths, model_config, training_config, checkpoint_index\n",
    "    )\n",
    "except (ValueError, OSError) as e:\n",
    "    print(f\"⚠️ Warning: Could not load model checkpoint: {str(e)[:200]}\")\n",
    "    print(\"This is expected if the model architecture has changed since the weights were saved.\")\n",
    "    print(\"Predictions will be skipped, but analysis of training logs will continue.\\n\")\n",
    "\n",
    "# Generate predictions (only if model loaded successfully and no scaler mismatch)\n",
    "residuals_df = pd.DataFrame()\n",
    "residual_metrics = {}\n",
    "\n",
    "if model is not None and not scaler_features_mismatch:\n",
    "    try:\n",
    "        residuals_df, residual_metrics = utils.compute_predictions(\n",
    "            model, scaler_X, scaler_y, particle_df, sample_size=SAMPLE_SIZE\n",
    "        )\n",
    "        if residuals_df.empty:\n",
    "            print(\"⚠️ Warning: Predictions returned empty results.\")\n",
    "            print(\"This may be due to missing derived features or data compatibility issues.\\n\")\n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"⚠️ Warning: Could not generate predictions: {str(e)[:100]}\")\n",
    "        print(\"Scaler or model compatibility issue detected.\\n\")\n",
    "elif model is not None and scaler_features_mismatch:\n",
    "    print(\"⚠️ Skipping predictions due to scaler feature mismatch.\")\n",
    "    print(\"Run a new training session to generate compatible scalers and model checkpoints.\\n\")\n",
    "\n",
    "print(\"✓ All data loaded successfully\\n\")\n",
    "\n",
    "# Data quality summary\n",
    "summary_stats = pd.DataFrame([\n",
    "    {\"Dataset\": \"Configuration history\", \"Records\": len(config_history), \"Status\": \"✓\"},\n",
    "    {\"Dataset\": \"Loss records\", \"Records\": len(loss_records), \"Status\": \"✓\"},\n",
    "    {\"Dataset\": \"Training results\", \"Records\": len(results_df), \"Status\": \"✓\"},\n",
    "    {\"Dataset\": \"Particle data\", \"Records\": len(particle_df), \"Status\": \"✓\"},\n",
    "    {\"Dataset\": \"Weight checkpoints\", \"Records\": len(checkpoint_index), \"Status\": \"✓\"},\n",
    "    {\"Dataset\": \"Residual predictions\", \"Records\": len(residuals_df) if not residuals_df.empty else 0, \n",
    "     \"Status\": \"✓\" if not residuals_df.empty else \"⚠️ (Model not loaded or data mismatch)\"}\n",
    "])\n",
    "\n",
    "display(Markdown(\"### Data Loading Summary\"))\n",
    "display(summary_stats)\n",
    "\n",
    "if checkpoint_meta:\n",
    "    display(Markdown(f\"\\n**Loaded model checkpoint:** Epoch {checkpoint_meta['epoch']} | \"\n",
    "                     f\"Parameters: {checkpoint_meta['parameter_count']:,}\"))\n",
    "elif not checkpoint_index.empty:\n",
    "    display(Markdown(\"\\n⚠️ **Model checkpoint available but could not be loaded due to architecture mismatch.**\"))\n",
    "    display(Markdown(\"Run a new training session with the current configuration to generate compatible checkpoints.\"))\n",
    "\n",
    "if scaler_features_mismatch:\n",
    "    display(Markdown(\"\\n⚠️ **Scaler feature mismatch detected.** Scalers will be regenerated for baseline comparisons.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3e850",
   "metadata": {},
   "source": [
    "### Quick Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2eb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_checks = []\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = particle_df.isna().sum().sum()\n",
    "quality_checks.append({\n",
    "    \"Check\": \"No missing values in particle data\",\n",
    "    \"Result\": \"PASS\" if missing_count == 0 else \"FAIL\",\n",
    "    \"Details\": f\"{missing_count} missing values\"\n",
    "})\n",
    "\n",
    "# Check training convergence\n",
    "final_val_loss = results_df.iloc[-1][\"val_loss\"]\n",
    "best_val_loss = results_df[\"val_loss\"].min()\n",
    "convergence_gap = final_val_loss - best_val_loss\n",
    "quality_checks.append({\n",
    "    \"Check\": \"Training convergence (final vs best < 0.01)\",\n",
    "    \"Result\": \"PASS\" if convergence_gap < 0.01 else \"WARNING\",\n",
    "    \"Details\": f\"Gap: {convergence_gap:.4f}\"\n",
    "})\n",
    "\n",
    "# Check train/val gap\n",
    "final_gap = results_df.iloc[-1][\"train_val_gap\"]\n",
    "quality_checks.append({\n",
    "    \"Check\": \"No severe overfitting (train/val gap < 0.1)\",\n",
    "    \"Result\": \"PASS\" if final_gap < 0.1 else \"WARNING\",\n",
    "    \"Details\": f\"Gap: {final_gap:.4f}\"\n",
    "})\n",
    "\n",
    "# Check checkpoint availability\n",
    "quality_checks.append({\n",
    "    \"Check\": \"Model checkpoint loaded successfully\",\n",
    "    \"Result\": \"PASS\" if model is not None else \"WARNING\",\n",
    "    \"Details\": f\"Epoch {checkpoint_meta['epoch'] if checkpoint_meta else 'Architecture mismatch'}\"\n",
    "})\n",
    "\n",
    "# Check prediction quality (only if model loaded)\n",
    "if residual_metrics and model is not None:\n",
    "    mae = residual_metrics.get(\"mae\", float('inf'))\n",
    "    quality_checks.append({\n",
    "        \"Check\": \"Reasonable prediction accuracy (MAE < 1.0)\",\n",
    "        \"Result\": \"PASS\" if mae < 1.0 else \"WARNING\",\n",
    "        \"Details\": f\"MAE: {mae:.4f}\"\n",
    "    })\n",
    "else:\n",
    "    quality_checks.append({\n",
    "        \"Check\": \"Prediction metrics available\",\n",
    "        \"Result\": \"SKIPPED\",\n",
    "        \"Details\": \"Model not loaded - predictions not available\"\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_checks)\n",
    "\n",
    "display(Markdown(\"### Data Quality Checks\"))\n",
    "display(quality_df)\n",
    "\n",
    "pass_count = (quality_df[\"Result\"] == \"PASS\").sum()\n",
    "print(f\"\\n{pass_count}/{len(quality_checks)} checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb87ae8",
   "metadata": {},
   "source": [
    "## 3. Training Dynamics Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ff33c",
   "metadata": {},
   "source": [
    "### Comprehensive Training Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Subplot 1: Training and validation loss curves with confidence bands\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(results_df[\"epoch\"], results_df[\"train_loss\"], label=\"Train Loss\", linewidth=2, color=\"#2E86AB\")\n",
    "ax1.plot(results_df[\"epoch\"], results_df[\"val_loss\"], label=\"Validation Loss\", linewidth=2, color=\"#A23B72\")\n",
    "\n",
    "# Add rolling standard deviation as confidence band\n",
    "val_std = results_df[\"val_loss\"].rolling(10, min_periods=1).std()\n",
    "val_mean = results_df[\"val_loss\"].rolling(10, min_periods=1).mean()\n",
    "ax1.fill_between(results_df[\"epoch\"], val_mean - val_std, val_mean + val_std, \n",
    "                  color=\"#A23B72\", alpha=0.15, label=\"Val Loss ±1 SD\")\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax1.set_title(\"Training and Validation Loss Progression\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"best\", frameon=True)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: R² score progression\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(results_df[\"epoch\"], results_df[\"r2_score\"], label=\"R² Score\", \n",
    "         linewidth=2, color=\"#F18F01\", marker=\"o\", markersize=3, markevery=10)\n",
    "ax2.axhline(results_df[\"r2_score\"].max(), linestyle=\"--\", color=\"green\", \n",
    "            linewidth=1.5, label=f\"Best R² = {results_df['r2_score'].max():.4f}\")\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax2.set_ylabel(\"R² Score\", fontsize=11)\n",
    "ax2.set_title(\"Model Performance (R² Score)\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.legend(loc=\"best\", frameon=True)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Epoch time and memory usage\n",
    "ax3 = axes[1, 0]\n",
    "ax3_twin = ax3.twinx()\n",
    "\n",
    "line1 = ax3.plot(results_df[\"epoch\"], results_df[\"epoch_time\"], \n",
    "                 label=\"Epoch Time\", linewidth=2, color=\"#06A77D\")\n",
    "line2 = ax3_twin.plot(results_df[\"epoch\"], results_df[\"memory_mb\"], \n",
    "                      label=\"Memory Usage\", linewidth=2, color=\"#D62246\", linestyle=\"--\")\n",
    "\n",
    "ax3.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax3.set_ylabel(\"Epoch Time (seconds)\", fontsize=11, color=\"#06A77D\")\n",
    "ax3_twin.set_ylabel(\"Memory Usage (MB)\", fontsize=11, color=\"#D62246\")\n",
    "ax3.set_title(\"Computational Resource Usage\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc=\"best\", frameon=True)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='y', labelcolor=\"#06A77D\")\n",
    "ax3_twin.tick_params(axis='y', labelcolor=\"#D62246\")\n",
    "\n",
    "# Subplot 4: Train/val gap evolution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(results_df[\"epoch\"], results_df[\"train_val_gap\"], \n",
    "         linewidth=2, color=\"#9B4F96\")\n",
    "ax4.axhline(0, linestyle=\"-\", color=\"black\", linewidth=1, alpha=0.3)\n",
    "ax4.fill_between(results_df[\"epoch\"], 0, results_df[\"train_val_gap\"], \n",
    "                  where=(results_df[\"train_val_gap\"] > 0), \n",
    "                  color=\"#E63946\", alpha=0.3, label=\"Overfitting Region\")\n",
    "ax4.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax4.set_ylabel(\"Val Loss - Train Loss\", fontsize=11)\n",
    "ax4.set_title(\"Generalization Gap Evolution\", fontsize=13, fontweight=\"bold\")\n",
    "ax4.legend(loc=\"best\", frameon=True)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "dashboard_path = paths[\"figures_dir\"] / \"training_dynamics_dashboard.png\"\n",
    "plt.savefig(dashboard_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "display(Markdown(f\"✓ Saved training dynamics dashboard to `{dashboard_path.name}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014403fc",
   "metadata": {},
   "source": [
    "### Learning Rate and Loss Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e777cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config_history.empty and \"learning_rate\" in config_history.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Learning rate vs final R²\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        config_history[\"learning_rate\"],\n",
    "        config_history[\"final_r2\"],\n",
    "        s=config_history.get(\"total_training_time\", 100) / 10,\n",
    "        c=config_history[\"final_r2\"],\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax1.set_xlabel(\"Learning Rate\", fontsize=11)\n",
    "    ax1.set_ylabel(\"Final R²\", fontsize=11)\n",
    "    ax1.set_title(\"Learning Rate Impact on Performance\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label=\"Final R²\")\n",
    "    \n",
    "    # Learning rate vs training time\n",
    "    if \"total_training_time\" in config_history.columns:\n",
    "        ax2 = axes[1]\n",
    "        ax2.scatter(\n",
    "            config_history[\"learning_rate\"],\n",
    "            config_history[\"total_training_time\"],\n",
    "            s=100,\n",
    "            c=config_history[\"best_r2\"],\n",
    "            cmap=\"coolwarm\",\n",
    "            alpha=0.7,\n",
    "            edgecolors=\"black\",\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        ax2.set_xlabel(\"Learning Rate\", fontsize=11)\n",
    "        ax2.set_ylabel(\"Total Training Time (s)\", fontsize=11)\n",
    "        ax2.set_title(\"Learning Rate vs Training Efficiency\", fontsize=13, fontweight=\"bold\")\n",
    "        ax2.set_xscale(\"log\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    lr_analysis_path = paths[\"figures_dir\"] / \"learning_rate_analysis.png\"\n",
    "    plt.savefig(lr_analysis_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(f\"✓ Saved learning rate analysis to `{lr_analysis_path.name}`\"))\n",
    "else:\n",
    "    print(\"⚠ Insufficient historical data for learning rate analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbaea9",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9f552",
   "metadata": {},
   "source": [
    "### Comprehensive Metric Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary = utils.summarize_run_performance(results_df, epoch_summary)\n",
    "\n",
    "display(Markdown(\"### Key Performance Indicators\"))\n",
    "display(performance_summary)\n",
    "\n",
    "if residual_metrics and model is not None:\n",
    "    display(Markdown(\"### Overall Residual Metrics\"))\n",
    "    overall_metrics = {k: v for k, v in residual_metrics.items() if k != \"targets\"}\n",
    "    display(pd.Series(overall_metrics, name=\"value\"))\n",
    "    \n",
    "    display(Markdown(\"### Per-Target Performance Breakdown\"))\n",
    "    target_metrics_df = pd.DataFrame(residual_metrics[\"targets\"]).T\n",
    "    target_metrics_df[\"relative_error\"] = target_metrics_df[\"mae\"] / (target_metrics_df[\"rmse\"] + 1e-8)\n",
    "    target_metrics_df = target_metrics_df.sort_values(\"mae\", ascending=False)\n",
    "    display(target_metrics_df)\n",
    "else:\n",
    "    display(Markdown(\"### Prediction Metrics\"))\n",
    "    print(\"⚠️ Prediction metrics not available - model checkpoint could not be loaded.\")\n",
    "    print(\"Run a new training session with the current configuration to generate compatible checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1d1f2",
   "metadata": {},
   "source": [
    "### Residual Analysis Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not residuals_df.empty and model is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Subplot 1: Residual distribution histogram with normal curve\n",
    "    ax1 = axes[0, 0]\n",
    "    residual_norms = residuals_df[\"residual_norm\"]\n",
    "    ax1.hist(residual_norms, bins=40, density=True, alpha=0.7, color=\"#4ECDC4\", edgecolor=\"black\")\n",
    "    \n",
    "    # Fit and plot normal distribution\n",
    "    mu, sigma = residual_norms.mean(), residual_norms.std()\n",
    "    x = np.linspace(residual_norms.min(), residual_norms.max(), 100)\n",
    "    ax1.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Normal(μ={mu:.3f}, σ={sigma:.3f})')\n",
    "    \n",
    "    ax1.set_xlabel(\"Residual Norm\", fontsize=11)\n",
    "    ax1.set_ylabel(\"Density\", fontsize=11)\n",
    "    ax1.set_title(\"Residual Distribution with Normal Fit\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.legend(loc=\"best\", frameon=True)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Q-Q plot for normality testing\n",
    "    ax2 = axes[0, 1]\n",
    "    stats.probplot(residual_norms, dist=\"norm\", plot=ax2)\n",
    "    ax2.set_title(\"Q-Q Plot (Normality Test)\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Residuals vs predicted values\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calculate average predicted value across all targets\n",
    "    pred_cols = [col for col in residuals_df.columns if col.startswith(\"pred_\")]\n",
    "    predicted_avg = residuals_df[pred_cols].mean(axis=1)\n",
    "    \n",
    "    scatter = ax3.scatter(predicted_avg, residual_norms, \n",
    "                         c=residual_norms, cmap=\"RdYlBu_r\", \n",
    "                         alpha=0.6, edgecolors=\"black\", linewidth=0.3)\n",
    "    ax3.axhline(0, linestyle=\"--\", color=\"black\", linewidth=1, alpha=0.5)\n",
    "    ax3.set_xlabel(\"Average Predicted Value\", fontsize=11)\n",
    "    ax3.set_ylabel(\"Residual Norm\", fontsize=11)\n",
    "    ax3.set_title(\"Residuals vs Predicted Values\", fontsize=13, fontweight=\"bold\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax3, label=\"Residual Norm\")\n",
    "    \n",
    "    # Subplot 4: Per-target residual boxplots\n",
    "    ax4 = axes[1, 1]\n",
    "    residual_cols = [col for col in residuals_df.columns \n",
    "                     if col.startswith(\"residual_\") and not col.endswith((\"norm\", \"_z\"))]\n",
    "    \n",
    "    residual_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for col in residual_cols:\n",
    "        residual_data.append(residuals_df[col])\n",
    "        # Extract target name (remove 'residual_' prefix)\n",
    "        labels.append(col.replace(\"residual_\", \"\"))\n",
    "    \n",
    "    bp = ax4.boxplot(residual_data, labels=labels, patch_artist=True, \n",
    "                     notch=True, showfliers=False)\n",
    "    \n",
    "    # Color boxes\n",
    "    colors = sns.color_palette(\"Set3\", len(residual_data))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax4.axhline(0, linestyle=\"--\", color=\"black\", linewidth=1, alpha=0.5)\n",
    "    ax4.set_xlabel(\"Target Variable\", fontsize=11)\n",
    "    ax4.set_ylabel(\"Residual\", fontsize=11)\n",
    "    ax4.set_title(\"Residual Distribution by Target\", fontsize=13, fontweight=\"bold\")\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    residual_suite_path = paths[\"figures_dir\"] / \"residual_analysis_suite.png\"\n",
    "    plt.savefig(residual_suite_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(f\"✓ Saved residual analysis suite to `{residual_suite_path.name}`\"))\n",
    "    \n",
    "    # Normality test\n",
    "    _, p_value = stats.shapiro(residual_norms[:min(len(residual_norms), 5000)])\n",
    "    normality_result = \"normally distributed\" if p_value > 0.05 else \"NOT normally distributed\"\n",
    "    print(f\"\\nShapiro-Wilk normality test: p-value = {p_value:.4f}\")\n",
    "    print(f\"Residuals appear to be {normality_result} (α=0.05)\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping residual analysis - model checkpoint not loaded or no predictions available.\")\n",
    "    print(\"Run a new training session with the current configuration to enable residual analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f83d3d",
   "metadata": {},
   "source": [
    "### Prediction Scatter Plots (Actual vs Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not residuals_df.empty and model is not None:\n",
    "    # Get unique target names\n",
    "    targets = [col.replace(\"actual_\", \"\") for col in residuals_df.columns if col.startswith(\"actual_\")]\n",
    "    \n",
    "    # Create subplots (2 rows, 3 columns for 6 targets)\n",
    "    n_targets = len(targets)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_targets + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if n_targets > 1 else [axes]\n",
    "    \n",
    "    for idx, target in enumerate(targets):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        actual_col = f\"actual_{target}\"\n",
    "        pred_col = f\"pred_{target}\"\n",
    "        \n",
    "        actual = residuals_df[actual_col]\n",
    "        predicted = residuals_df[pred_col]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(actual, predicted, alpha=0.5, s=20, edgecolors=\"none\", color=\"#3A86FF\")\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(actual.min(), predicted.min())\n",
    "        max_val = max(actual.max(), predicted.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label=\"Perfect Prediction\")\n",
    "        \n",
    "        # Calculate R²\n",
    "        r2 = r2_score(actual, predicted)\n",
    "        mae = mean_absolute_error(actual, predicted)\n",
    "        \n",
    "        ax.set_xlabel(f\"Actual {target}\", fontsize=10)\n",
    "        ax.set_ylabel(f\"Predicted {target}\", fontsize=10)\n",
    "        ax.set_title(f\"{target}\\nR² = {r2:.4f}, MAE = {mae:.4f}\", fontsize=11, fontweight=\"bold\")\n",
    "        ax.legend(loc=\"best\", frameon=True, fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_targets, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    scatter_path = paths[\"figures_dir\"] / \"prediction_scatter_plots.png\"\n",
    "    plt.savefig(scatter_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(f\"✓ Saved prediction scatter plots to `{scatter_path.name}`\"))\n",
    "else:\n",
    "    print(\"⚠️ Skipping prediction scatter plots - model checkpoint not loaded or no predictions available.\")\n",
    "    print(\"Run a new training session with the current configuration to enable prediction analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c9f49",
   "metadata": {},
   "source": [
    "## 5. Benchmarking and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c702c2",
   "metadata": {},
   "source": [
    "### Baseline Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b719611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comparison with baseline models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "X = particle_df[utils.INPUT_FEATURES].values\n",
    "y = particle_df[utils.OUTPUT_TARGETS].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=ANALYSIS_SEED\n",
    ")\n",
    "\n",
    "# Scale data - handle scaler mismatch by refitting if necessary\n",
    "try:\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "except ValueError as e:\n",
    "    print(f\"⚠️ Warning: Scaler feature mismatch detected: {str(e)[:100]}\")\n",
    "    print(\"Refitting scalers with current data...\\n\")\n",
    "    \n",
    "    # Refit scalers with current data\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Mean baseline\n",
    "mean_predictions = np.tile(y_train.mean(axis=0), (len(y_test), 1))\n",
    "\n",
    "# Linear regression baseline\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Current model predictions (only if model loaded successfully)\n",
    "if model is not None:\n",
    "    try:\n",
    "        model_predictions_scaled = model.predict(X_test_scaled, verbose=0)\n",
    "        model_predictions = scaler_y.inverse_transform(model_predictions_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not generate predictions with loaded model: {str(e)[:100]}\")\n",
    "        model_predictions = mean_predictions\n",
    "        model = None  # Mark as unavailable for comparison\n",
    "else:\n",
    "    model_predictions = mean_predictions\n",
    "\n",
    "# Compute metrics for each baseline\n",
    "baseline_results = []\n",
    "\n",
    "for name, preds in [\n",
    "    (\"Mean Baseline\", mean_predictions),\n",
    "    (\"Linear Regression\", lr_predictions),\n",
    "    (\"Advanced NN (Binary Constraints)\", model_predictions)\n",
    "]:\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mape = np.mean(np.abs((y_test - preds) / (y_test + 1e-8))) * 100\n",
    "    \n",
    "    baseline_results.append({\n",
    "        \"Model\": name if name != \"Advanced NN (Binary Constraints)\" or model is not None else f\"{name} (unavailable)\",\n",
    "        \"R²\": r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE (%)\": mape\n",
    "    })\n",
    "\n",
    "baseline_comparison_df = pd.DataFrame(baseline_results)\n",
    "baseline_comparison_df = baseline_comparison_df.sort_values(\"R²\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(Markdown(\"### Baseline Model Comparison\"))\n",
    "display(baseline_comparison_df)\n",
    "\n",
    "if model is None:\n",
    "    display(Markdown(\"\\n⚠️ **Note:** Advanced NN comparison uses mean baseline as model checkpoint could not be loaded.\"))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = [\"R²\", \"MAE\", \"RMSE\"]\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    bars = ax.barh(baseline_comparison_df[\"Model\"], baseline_comparison_df[metric], \n",
    "                   color=[\"#E63946\", \"#F1FAEE\", \"#A8DADC\"])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=11)\n",
    "    ax.set_title(f\"{metric} Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "baseline_comp_path = paths[\"figures_dir\"] / \"baseline_comparison.png\"\n",
    "plt.savefig(baseline_comp_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "display(Markdown(f\"✓ Saved baseline comparison to `{baseline_comp_path.name}`\"))\n",
    "\n",
    "# Performance improvement calculation (only if model loaded)\n",
    "if model is not None:\n",
    "    baseline_r2 = baseline_comparison_df.loc[baseline_comparison_df[\"Model\"] == \"Mean Baseline\", \"R²\"].values[0]\n",
    "    model_r2 = baseline_comparison_df.loc[baseline_comparison_df[\"Model\"] == \"Advanced NN (Binary Constraints)\", \"R²\"].values[0]\n",
    "    improvement = ((model_r2 - baseline_r2) / (1 - baseline_r2)) * 100\n",
    "    \n",
    "    print(f\"\\nModel improvement over mean baseline: {improvement:.2f}% of possible improvement\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Model improvement calculation skipped - checkpoint not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d2264",
   "metadata": {},
   "source": [
    "### James-Stein Estimator Comparison\n",
    "\n",
    "Train three model variants with identical hyperparameters to rigorously evaluate weight constraint effectiveness:\n",
    "1. **Binary Weight Constraints** (current approach)\n",
    "2. **James-Stein Shrinkage** (statistical benchmark)\n",
    "3. **No Constraints** (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29261a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three model variants for comparison\n",
    "from advanced_neural_network import AdvancedNeuralNetwork\n",
    "from james_stein_weight_constraint import create_james_stein_model\n",
    "\n",
    "print(\"Training model variants for James-Stein comparison...\")\n",
    "print(\"This may take several minutes depending on dataset size and epochs.\\n\")\n",
    "\n",
    "# Common hyperparameters\n",
    "common_config = {\n",
    "    \"hidden_layers\": model_config.get(\"hidden_layers\", [64, 32, 16]),\n",
    "    \"activation\": model_config.get(\"activation\", \"relu\"),\n",
    "    \"dropout_rate\": model_config.get(\"dropout_rate\", 0.0),\n",
    "    \"learning_rate\": model_config.get(\"learning_rate\", 0.001),\n",
    "    \"epochs\": min(training_config.get(\"epochs\", 50), 50),  # Limit to 50 epochs for comparison\n",
    "    \"batch_size\": training_config.get(\"batch_size\", 16),\n",
    "    \"validation_split\": 0.2,\n",
    "    \"use_batch_norm\": model_config.get(\"use_batch_norm\", False),\n",
    "    \"batch_norm_momentum\": model_config.get(\"batch_norm_momentum\", 0.99),\n",
    "    \"l2_regularization\": model_config.get(\"l2_regularization\", 0.0),\n",
    "    \"loss_weighting_strategy\": model_config.get(\"loss_weighting_strategy\", \"none\")\n",
    "}\n",
    "\n",
    "input_shape = (len(utils.INPUT_FEATURES),)\n",
    "output_shape = len(utils.OUTPUT_TARGETS)\n",
    "\n",
    "# Prepare training data\n",
    "X_train_full = scaler_X.transform(particle_df[utils.INPUT_FEATURES].values)\n",
    "y_train_full = scaler_y.transform(particle_df[utils.OUTPUT_TARGETS].values)\n",
    "\n",
    "# Split into train/val\n",
    "val_split = int(len(X_train_full) * 0.8)\n",
    "X_train_comp = X_train_full[:val_split]\n",
    "X_val_comp = X_train_full[val_split:]\n",
    "y_train_comp = y_train_full[:val_split]\n",
    "y_val_comp = y_train_full[val_split:]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "# Model 1: Binary Weight Constraints\n",
    "print(\"1/3: Training Binary Weight Constraints model...\")\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(ANALYSIS_SEED)\n",
    "\n",
    "config_binary = common_config.copy()\n",
    "config_binary.update({\n",
    "    \"enable_binary_change_max\": model_config.get(\"enable_binary_change_max\", True),\n",
    "    \"max_additional_binary_digits\": model_config.get(\"max_additional_binary_digits\", 16),\n",
    "    \"enable_weight_oscillation_dampener\": model_config.get(\"enable_weight_oscillation_dampener\", True),\n",
    "    \"use_adaptive_oscillation_dampener\": model_config.get(\"use_adaptive_oscillation_dampener\", True),\n",
    "    \"constraint_interval\": model_config.get(\"constraint_interval\", 10)\n",
    "})\n",
    "\n",
    "model_binary = AdvancedNeuralNetwork(input_shape, output_shape, config_binary)\n",
    "\n",
    "# Compile model with loss and metrics\n",
    "model_binary.model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=common_config[\"learning_rate\"]),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "history_binary = model_binary.model.fit(\n",
    "    X_train_comp, y_train_comp,\n",
    "    validation_data=(X_val_comp, y_val_comp),\n",
    "    epochs=common_config[\"epochs\"],\n",
    "    batch_size=common_config[\"batch_size\"],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "comparison_results[\"Binary Constraints\"] = {\n",
    "    \"model\": model_binary.model,\n",
    "    \"history\": history_binary,\n",
    "    \"final_val_loss\": history_binary.history[\"val_loss\"][-1],\n",
    "    \"final_val_mae\": history_binary.history[\"val_mae\"][-1],\n",
    "    \"best_val_loss\": min(history_binary.history[\"val_loss\"]),\n",
    "    \"convergence_epoch\": np.argmin(history_binary.history[\"val_loss\"]) + 1\n",
    "}\n",
    "\n",
    "print(f\"   ✓ Final validation loss: {comparison_results['Binary Constraints']['final_val_loss']:.4f}\\n\")\n",
    "\n",
    "# Model 2: James-Stein Shrinkage\n",
    "print(\"2/3: Training James-Stein Shrinkage model...\")\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(ANALYSIS_SEED)\n",
    "\n",
    "model_js = create_james_stein_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    hidden_layers=common_config[\"hidden_layers\"],\n",
    "    activation=common_config[\"activation\"],\n",
    "    dropout_rate=common_config[\"dropout_rate\"],\n",
    "    learning_rate=common_config[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "history_js = model_js.fit(\n",
    "    X_train_comp, y_train_comp,\n",
    "    validation_data=(X_val_comp, y_val_comp),\n",
    "    epochs=common_config[\"epochs\"],\n",
    "    batch_size=common_config[\"batch_size\"],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "comparison_results[\"James-Stein\"] = {\n",
    "    \"model\": model_js,\n",
    "    \"history\": history_js,\n",
    "    \"final_val_loss\": history_js.history[\"val_loss\"][-1],\n",
    "    \"final_val_mae\": history_js.history[\"val_mae\"][-1],\n",
    "    \"best_val_loss\": min(history_js.history[\"val_loss\"]),\n",
    "    \"convergence_epoch\": np.argmin(history_js.history[\"val_loss\"]) + 1\n",
    "}\n",
    "\n",
    "print(f\"   ✓ Final validation loss: {comparison_results['James-Stein']['final_val_loss']:.4f}\\n\")\n",
    "\n",
    "# Model 3: No Constraints (Baseline)\n",
    "print(\"3/3: Training No Constraints baseline model...\")\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(ANALYSIS_SEED)\n",
    "\n",
    "model_baseline = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "])\n",
    "\n",
    "for units in common_config[\"hidden_layers\"]:\n",
    "    model_baseline.add(tf.keras.layers.Dense(units, activation=common_config[\"activation\"]))\n",
    "    if common_config[\"dropout_rate\"] > 0:\n",
    "        model_baseline.add(tf.keras.layers.Dropout(common_config[\"dropout_rate\"]))\n",
    "\n",
    "model_baseline.add(tf.keras.layers.Dense(output_shape))\n",
    "\n",
    "model_baseline.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=common_config[\"learning_rate\"]),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "history_baseline = model_baseline.fit(\n",
    "    X_train_comp, y_train_comp,\n",
    "    validation_data=(X_val_comp, y_val_comp),\n",
    "    epochs=common_config[\"epochs\"],\n",
    "    batch_size=common_config[\"batch_size\"],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "comparison_results[\"No Constraints\"] = {\n",
    "    \"model\": model_baseline,\n",
    "    \"history\": history_baseline,\n",
    "    \"final_val_loss\": history_baseline.history[\"val_loss\"][-1],\n",
    "    \"final_val_mae\": history_baseline.history[\"val_mae\"][-1],\n",
    "    \"best_val_loss\": min(history_baseline.history[\"val_loss\"]),\n",
    "    \"convergence_epoch\": np.argmin(history_baseline.history[\"val_loss\"]) + 1\n",
    "}\n",
    "\n",
    "print(f\"   ✓ Final validation loss: {comparison_results['No Constraints']['final_val_loss']:.4f}\\n\")\n",
    "\n",
    "print(\"✓ All models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19700af",
   "metadata": {},
   "source": [
    "### James-Stein Comparison Visualization and Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ef51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Subplot 1: Convergence comparison\n",
    "ax1 = axes[0, 0]\n",
    "for name, data in comparison_results.items():\n",
    "    ax1.plot(data[\"history\"].history[\"val_loss\"], label=name, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax1.set_ylabel(\"Validation Loss\", fontsize=11)\n",
    "ax1.set_title(\"Convergence Speed Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"best\", frameon=True)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Performance metrics comparison\n",
    "ax2 = axes[0, 1]\n",
    "metrics_data = {\n",
    "    \"Model\": list(comparison_results.keys()),\n",
    "    \"Final Val Loss\": [data[\"final_val_loss\"] for data in comparison_results.values()],\n",
    "    \"Best Val Loss\": [data[\"best_val_loss\"] for data in comparison_results.values()],\n",
    "    \"Convergence Epoch\": [data[\"convergence_epoch\"] for data in comparison_results.values()]\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_data[\"Model\"]))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, metrics_data[\"Final Val Loss\"], width, \n",
    "                label=\"Final Val Loss\", color=\"#3A86FF\", alpha=0.8)\n",
    "bars2 = ax2.bar(x + width/2, metrics_data[\"Best Val Loss\"], width, \n",
    "                label=\"Best Val Loss\", color=\"#FB5607\", alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel(\"Model\", fontsize=11)\n",
    "ax2.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax2.set_title(\"Performance Metrics\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics_data[\"Model\"], rotation=15, ha=\"right\")\n",
    "ax2.legend(loc=\"best\", frameon=True)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 3: Generalization gap\n",
    "ax3 = axes[0, 2]\n",
    "gen_gaps = []\n",
    "for name, data in comparison_results.items():\n",
    "    final_train = data[\"history\"].history[\"loss\"][-1]\n",
    "    final_val = data[\"final_val_loss\"]\n",
    "    gen_gaps.append(final_val - final_train)\n",
    "\n",
    "bars = ax3.barh(list(comparison_results.keys()), gen_gaps, \n",
    "                color=[\"#06A77D\" if gap < 0.02 else \"#E63946\" for gap in gen_gaps])\n",
    "\n",
    "for i, (bar, gap) in enumerate(zip(bars, gen_gaps)):\n",
    "    ax3.text(gap + 0.001, i, f\"{gap:.4f}\", va='center', fontsize=10)\n",
    "\n",
    "ax3.axvline(0, color='black', linewidth=1, linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel(\"Val Loss - Train Loss\", fontsize=11)\n",
    "ax3.set_title(\"Generalization Gap\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Subplot 4: Weight distribution histograms\n",
    "ax4 = axes[1, 0]\n",
    "for name, data in comparison_results.items():\n",
    "    weights = []\n",
    "    for layer in data[\"model\"].layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            weights.extend(layer.kernel.numpy().flatten())\n",
    "    \n",
    "    ax4.hist(weights, bins=50, alpha=0.5, label=name, density=True)\n",
    "\n",
    "ax4.set_xlabel(\"Weight Value\", fontsize=11)\n",
    "ax4.set_ylabel(\"Density\", fontsize=11)\n",
    "ax4.set_title(\"Weight Distribution Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "ax4.legend(loc=\"best\", frameon=True)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 5: Weight sparsity comparison\n",
    "ax5 = axes[1, 1]\n",
    "sparsity_data = []\n",
    "for name, data in comparison_results.items():\n",
    "    weights = []\n",
    "    for layer in data[\"model\"].layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            weights.extend(layer.kernel.numpy().flatten())\n",
    "    \n",
    "    # Calculate sparsity (percentage of weights near zero)\n",
    "    near_zero = np.abs(weights) < 1e-3\n",
    "    sparsity = np.mean(near_zero) * 100\n",
    "    sparsity_data.append(sparsity)\n",
    "\n",
    "bars = ax5.bar(list(comparison_results.keys()), sparsity_data, \n",
    "               color=[\"#4361EE\", \"#F72585\", \"#4CC9F0\"])\n",
    "\n",
    "for bar, val in zip(bars, sparsity_data):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f\"{val:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "ax5.set_ylabel(\"Sparsity (%)\", fontsize=11)\n",
    "ax5.set_title(\"Weight Sparsity (|w| < 0.001)\", fontsize=12, fontweight=\"bold\")\n",
    "ax5.set_xticklabels(list(comparison_results.keys()), rotation=15, ha=\"right\")\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 6: Training stability (loss variance)\n",
    "ax6 = axes[1, 2]\n",
    "stability_data = []\n",
    "for name, data in comparison_results.items():\n",
    "    loss_history = data[\"history\"].history[\"val_loss\"]\n",
    "    # Compute coefficient of variation for last 20% of training\n",
    "    last_losses = loss_history[int(len(loss_history) * 0.8):]\n",
    "    cv = np.std(last_losses) / (np.mean(last_losses) + 1e-8)\n",
    "    stability_data.append(cv)\n",
    "\n",
    "bars = ax6.bar(list(comparison_results.keys()), stability_data, \n",
    "               color=[\"#06A77D\", \"#FFB703\", \"#FB8500\"])\n",
    "\n",
    "for bar, val in zip(bars, stability_data):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f\"{val:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "ax6.set_ylabel(\"Coefficient of Variation\", fontsize=11)\n",
    "ax6.set_title(\"Training Stability (lower is better)\", fontsize=12, fontweight=\"bold\")\n",
    "ax6.set_xticklabels(list(comparison_results.keys()), rotation=15, ha=\"right\")\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "js_comparison_path = paths[\"figures_dir\"] / \"james_stein_comparison.png\"\n",
    "plt.savefig(js_comparison_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "display(Markdown(f\"✓ Saved James-Stein comparison to `{js_comparison_path.name}`\"))\n",
    "\n",
    "# Create summary table\n",
    "comparison_summary = pd.DataFrame({\n",
    "    \"Model\": list(comparison_results.keys()),\n",
    "    \"Best Val Loss\": [data[\"best_val_loss\"] for data in comparison_results.values()],\n",
    "    \"Final Val Loss\": [data[\"final_val_loss\"] for data in comparison_results.values()],\n",
    "    \"Final Val MAE\": [data[\"final_val_mae\"] for data in comparison_results.values()],\n",
    "    \"Convergence Epoch\": [data[\"convergence_epoch\"] for data in comparison_results.values()],\n",
    "    \"Generalization Gap\": gen_gaps,\n",
    "    \"Weight Sparsity (%)\": sparsity_data,\n",
    "    \"Training Stability (CV)\": stability_data\n",
    "})\n",
    "\n",
    "display(Markdown(\"### James-Stein Comparison Summary Table\"))\n",
    "display(comparison_summary)\n",
    "\n",
    "# Statistical significance testing (Wilcoxon signed-rank test on validation losses)\n",
    "if len(comparison_results) >= 2:\n",
    "    display(Markdown(\"### Statistical Significance Testing\"))\n",
    "    \n",
    "    # Compare Binary Constraints vs James-Stein\n",
    "    bc_losses = comparison_results[\"Binary Constraints\"][\"history\"].history[\"val_loss\"]\n",
    "    js_losses = comparison_results[\"James-Stein\"][\"history\"].history[\"val_loss\"]\n",
    "    \n",
    "    # Align lengths\n",
    "    min_len = min(len(bc_losses), len(js_losses))\n",
    "    bc_losses = bc_losses[:min_len]\n",
    "    js_losses = js_losses[:min_len]\n",
    "    \n",
    "    # Wilcoxon signed-rank test\n",
    "    statistic, p_value = stats.wilcoxon(bc_losses, js_losses)\n",
    "    \n",
    "    sig_test_results = pd.DataFrame([{\n",
    "        \"Comparison\": \"Binary Constraints vs James-Stein\",\n",
    "        \"Test\": \"Wilcoxon Signed-Rank\",\n",
    "        \"Statistic\": statistic,\n",
    "        \"P-Value\": p_value,\n",
    "        \"Significant (α=0.05)\": \"Yes\" if p_value < 0.05 else \"No\",\n",
    "        \"Interpretation\": \"Binary Constraints are significantly different\" if p_value < 0.05 \n",
    "                         else \"No significant difference detected\"\n",
    "    }])\n",
    "    \n",
    "    display(sig_test_results)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    mean_diff = np.mean(bc_losses) - np.mean(js_losses)\n",
    "    pooled_std = np.sqrt((np.std(bc_losses)**2 + np.std(js_losses)**2) / 2)\n",
    "    cohens_d = mean_diff / (pooled_std + 1e-8)\n",
    "    \n",
    "    print(f\"\\nEffect size (Cohen's d): {cohens_d:.4f}\")\n",
    "    print(f\"Mean difference: {mean_diff:.4f}\")\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    \n",
    "    print(f\"Effect interpretation: {effect_interpretation}\")\n",
    "\n",
    "# Recommendations based on comparison\n",
    "display(Markdown(\"### Recommendations\"))\n",
    "\n",
    "best_model = min(comparison_results.items(), key=lambda x: x[1][\"best_val_loss\"])\n",
    "print(f\"✓ Best performing model: {best_model[0]}\")\n",
    "print(f\"  - Best validation loss: {best_model[1]['best_val_loss']:.4f}\")\n",
    "print(f\"  - Converged at epoch: {best_model[1]['convergence_epoch']}\")\n",
    "\n",
    "print(\"\\n**When to use each approach:**\")\n",
    "print(\"• Binary Constraints: When discrete precision matters and you want controlled weight updates\")\n",
    "print(\"• James-Stein: When shrinkage toward zero is beneficial and you want statistical optimality\")\n",
    "print(\"• No Constraints: When full model expressiveness is critical and overfitting is not a concern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59915c",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d18aa7",
   "metadata": {},
   "source": [
    "### Advanced Hyperparameter Recommendations with Statistical Backing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_recommendations(\n",
    "    model_config: Dict[str, Any],\n",
    "    training_config: Dict[str, Any],\n",
    "    config_history: pd.DataFrame,\n",
    "    results_df: pd.DataFrame,\n",
    "    confidence_level: float = 0.95\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate statistically-backed hyperparameter recommendations.\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Learning rate sensitivity analysis\n",
    "    current_lr = float(model_config.get(\"learning_rate\", 0.001))\n",
    "    final_window = results_df.tail(10)\n",
    "    \n",
    "    # Check if we have enough valid data for polyfit\n",
    "    if len(final_window) >= 2 and not final_window[\"val_loss\"].isna().any():\n",
    "        try:\n",
    "            val_loss_trend = np.polyfit(range(len(final_window)), final_window[\"val_loss\"], 1)[0]\n",
    "            \n",
    "            if abs(val_loss_trend) < 0.001:  # Plateau detected\n",
    "                lr_candidates = [current_lr * 0.5, current_lr * 0.8, current_lr * 1.2, current_lr * 1.5]\n",
    "                \n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"learning_rate\",\n",
    "                    \"Current\": current_lr,\n",
    "                    \"Suggested\": lr_candidates,\n",
    "                    \"Confidence\": \"Medium\",\n",
    "                    \"Rationale\": \"Validation loss plateaued (slope ≈ 0). Try varying learning rate to escape local minimum.\",\n",
    "                    \"Statistical Evidence\": f\"Loss trend slope: {val_loss_trend:.6f}\",\n",
    "                    \"Expected Impact\": \"±10-30% validation loss change\",\n",
    "                    \"Priority\": \"High\"\n",
    "                })\n",
    "        except (np.linalg.LinAlgError, ValueError):\n",
    "            pass  # Skip if polyfit fails\n",
    "    \n",
    "    # 2. Overfitting detection with statistical test\n",
    "    if len(results_df) >= 10 and \"train_val_gap\" in results_df.columns:\n",
    "        train_val_gap = results_df.tail(10)[\"train_val_gap\"]\n",
    "        \n",
    "        if train_val_gap.mean() > 0.03:\n",
    "            # One-sample t-test against null hypothesis of zero gap\n",
    "            t_stat, p_value = stats.ttest_1samp(train_val_gap, 0)\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                current_dropout = float(model_config.get(\"dropout_rate\", 0.0))\n",
    "                dropout_candidates = [current_dropout + 0.05, current_dropout + 0.1, current_dropout + 0.15]\n",
    "                dropout_candidates = [min(d, 0.5) for d in dropout_candidates]  # Cap at 0.5\n",
    "                \n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"dropout_rate\",\n",
    "                    \"Current\": current_dropout,\n",
    "                    \"Suggested\": dropout_candidates,\n",
    "                    \"Confidence\": \"High\",\n",
    "                    \"Rationale\": f\"Significant train/val gap detected (mean={train_val_gap.mean():.4f}). Increase regularization.\",\n",
    "                    \"Statistical Evidence\": f\"t-statistic={t_stat:.3f}, p-value={p_value:.4f} (α=0.05)\",\n",
    "                    \"Expected Impact\": \"5-15% reduction in generalization gap\",\n",
    "                    \"Priority\": \"High\"\n",
    "                })\n",
    "    \n",
    "    # 3. Batch size optimization based on memory and time\n",
    "    if \"memory_headroom_mb\" in results_df.columns and \"epoch_time\" in results_df.columns:\n",
    "        avg_headroom = results_df.tail(10)[\"memory_headroom_mb\"].mean()\n",
    "        avg_epoch_time = results_df.tail(10)[\"epoch_time\"].mean()\n",
    "        \n",
    "        if avg_headroom > 500 and avg_epoch_time < 2.0:\n",
    "            current_batch = int(training_config.get(\"batch_size\", 16))\n",
    "            batch_candidates = [current_batch * 1.5, current_batch * 2, current_batch * 2.5]\n",
    "            batch_candidates = [int(b) for b in batch_candidates]\n",
    "            \n",
    "            recommendations.append({\n",
    "                \"Parameter\": \"batch_size\",\n",
    "                \"Current\": current_batch,\n",
    "                \"Suggested\": batch_candidates,\n",
    "                \"Confidence\": \"Medium\",\n",
    "                \"Rationale\": f\"Memory headroom available ({avg_headroom:.0f} MB) and fast epochs ({avg_epoch_time:.2f}s). Larger batches can improve training stability.\",\n",
    "                \"Statistical Evidence\": f\"Headroom > 500 MB, epoch time < 2s\",\n",
    "                \"Expected Impact\": \"10-25% faster training, potentially more stable convergence\",\n",
    "                \"Priority\": \"Medium\"\n",
    "            })\n",
    "    \n",
    "    # 4. Convergence analysis and epoch extension\n",
    "    if len(results_df) > 0 and \"val_loss\" in results_df.columns:\n",
    "        best_epoch = int(results_df.loc[results_df[\"val_loss\"].idxmin(), \"epoch\"])\n",
    "        total_epochs = int(results_df.iloc[-1][\"epoch\"])\n",
    "        current_max_epochs = int(training_config.get(\"epochs\", total_epochs))\n",
    "        \n",
    "        if best_epoch >= total_epochs * 0.8:  # Best epoch in last 20%\n",
    "            # Extrapolate improvement potential\n",
    "            if len(results_df) >= 20:\n",
    "                recent_improvement = results_df.tail(20)[\"val_loss\"].iloc[0] - results_df[\"val_loss\"].min()\n",
    "                \n",
    "                if recent_improvement > 0.01:\n",
    "                    epoch_candidates = [current_max_epochs + 20, current_max_epochs + 50, current_max_epochs + 100]\n",
    "                    \n",
    "                    recommendations.append({\n",
    "                        \"Parameter\": \"epochs\",\n",
    "                        \"Current\": current_max_epochs,\n",
    "                        \"Suggested\": epoch_candidates,\n",
    "                        \"Confidence\": \"High\",\n",
    "                        \"Rationale\": f\"Best epoch ({best_epoch}) near training end. Recent improvement: {recent_improvement:.4f}. Extend training.\",\n",
    "                        \"Statistical Evidence\": f\"Best epoch at {(best_epoch/total_epochs)*100:.1f}% of training\",\n",
    "                        \"Expected Impact\": \"Potential 5-20% additional loss reduction\",\n",
    "                        \"Priority\": \"High\"\n",
    "                    })\n",
    "    \n",
    "    # 5. Historical pattern recognition\n",
    "    if not config_history.empty and \"learning_rate\" in config_history.columns and \"final_r2\" in config_history.columns:\n",
    "        lr_perf = config_history.groupby(\"learning_rate\")[\"final_r2\"].agg(['mean', 'std', 'count'])\n",
    "        \n",
    "        if len(lr_perf) >= 3:  # Need at least 3 different learning rates\n",
    "            best_lr_row = lr_perf.loc[lr_perf['mean'].idxmax()]\n",
    "            best_historical_lr = best_lr_row.name\n",
    "            \n",
    "            if abs(best_historical_lr - current_lr) / current_lr > 0.3:  # >30% different\n",
    "                ci_margin = 1.96 * best_lr_row['std'] / np.sqrt(best_lr_row['count'])\n",
    "                \n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"learning_rate\",\n",
    "                    \"Current\": current_lr,\n",
    "                    \"Suggested\": [best_historical_lr],\n",
    "                    \"Confidence\": \"High\",\n",
    "                    \"Rationale\": f\"Historical data shows LR={best_historical_lr:.6f} achieves higher R² (mean={best_lr_row['mean']:.4f}).\",\n",
    "                    \"Statistical Evidence\": f\"95% CI for R²: [{best_lr_row['mean']-ci_margin:.4f}, {best_lr_row['mean']+ci_margin:.4f}]\",\n",
    "                    \"Expected Impact\": f\"R² improvement: {(best_lr_row['mean'] - results_df['r2_score'].iloc[-1]):.4f}\",\n",
    "                    \"Priority\": \"High\"\n",
    "                })\n",
    "    \n",
    "    # 6. Learning rate schedule recommendation\n",
    "    if len(results_df) >= 10 and \"val_loss\" in results_df.columns:\n",
    "        loss_variance = results_df[\"val_loss\"].rolling(10).std().mean()\n",
    "        \n",
    "        if loss_variance > 0.01:\n",
    "            recommendations.append({\n",
    "                \"Parameter\": \"learning_rate_schedule\",\n",
    "                \"Current\": \"None\",\n",
    "                \"Suggested\": [\"ExponentialDecay(initial={}, decay_rate=0.96, decay_steps=10)\".format(current_lr),\n",
    "                             \"CosineDecay(initial={}, decay_steps={})\".format(current_lr, training_config.get(\"epochs\", 100))],\n",
    "                \"Confidence\": \"Medium\",\n",
    "                \"Rationale\": f\"High loss variance detected (σ={loss_variance:.4f}). Adaptive LR schedule can smooth convergence.\",\n",
    "                \"Statistical Evidence\": f\"Loss variance > threshold (0.01)\",\n",
    "                \"Expected Impact\": \"Smoother convergence, 10-20% faster to optimal\",\n",
    "                \"Priority\": \"Medium\"\n",
    "            })\n",
    "    \n",
    "    # 7. Binary weight constraint optimization\n",
    "    if model_config.get(\"enable_binary_change_max\", False):\n",
    "        current_max_digits = model_config.get(\"max_additional_binary_digits\", 16)\n",
    "        \n",
    "        # Check if training is highly stable (might benefit from tighter constraints)\n",
    "        if len(results_df) >= 10:\n",
    "            loss_variance = results_df.tail(10)[\"val_loss\"].std()\n",
    "            \n",
    "            if loss_variance < 0.005 and current_max_digits > 8:\n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"max_additional_binary_digits\",\n",
    "                    \"Current\": current_max_digits,\n",
    "                    \"Suggested\": [current_max_digits // 2, max(4, current_max_digits // 4)],\n",
    "                    \"Confidence\": \"Medium\",\n",
    "                    \"Rationale\": f\"Training is highly stable (loss σ={loss_variance:.4f}). Tighter binary constraints may improve generalization.\",\n",
    "                    \"Statistical Evidence\": f\"Loss variance < 0.005\",\n",
    "                    \"Expected Impact\": \"Potentially better generalization with lower precision\",\n",
    "                    \"Priority\": \"Low\"\n",
    "                })\n",
    "            elif loss_variance > 0.02 and current_max_digits < 16:\n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"max_additional_binary_digits\",\n",
    "                    \"Current\": current_max_digits,\n",
    "                    \"Suggested\": [min(32, current_max_digits * 2)],\n",
    "                    \"Confidence\": \"Medium\",\n",
    "                    \"Rationale\": f\"Training is unstable (loss σ={loss_variance:.4f}). Looser binary constraints may help.\",\n",
    "                    \"Statistical Evidence\": f\"Loss variance > 0.02\",\n",
    "                    \"Expected Impact\": \"More flexibility in weight updates\",\n",
    "                    \"Priority\": \"Medium\"\n",
    "                })\n",
    "    \n",
    "    # 8. Weight oscillation dampener recommendation\n",
    "    if not model_config.get(\"enable_weight_oscillation_dampener\", False):\n",
    "        # Check if there are oscillations in the loss\n",
    "        if len(results_df) >= 10:\n",
    "            val_loss = results_df.tail(20)[\"val_loss\"]\n",
    "            # Count sign changes in the derivative\n",
    "            derivatives = val_loss.diff()\n",
    "            sign_changes = (derivatives.shift(1) * derivatives < 0).sum()\n",
    "            \n",
    "            if sign_changes > 5:  # Multiple oscillations detected\n",
    "                recommendations.append({\n",
    "                    \"Parameter\": \"enable_weight_oscillation_dampener\",\n",
    "                    \"Current\": False,\n",
    "                    \"Suggested\": [True],\n",
    "                    \"Confidence\": \"High\",\n",
    "                    \"Rationale\": f\"Detected {sign_changes} loss oscillations in last 20 epochs. Weight dampener can stabilize training.\",\n",
    "                    \"Statistical Evidence\": f\"Sign changes in loss derivative: {sign_changes}\",\n",
    "                    \"Expected Impact\": \"More stable convergence, reduced oscillations\",\n",
    "                    \"Priority\": \"High\"\n",
    "                })\n",
    "    \n",
    "    # 9. Adaptive oscillation dampener recommendation\n",
    "    if model_config.get(\"enable_weight_oscillation_dampener\", False) and not model_config.get(\"use_adaptive_oscillation_dampener\", False):\n",
    "        recommendations.append({\n",
    "            \"Parameter\": \"use_adaptive_oscillation_dampener\",\n",
    "            \"Current\": False,\n",
    "            \"Suggested\": [True],\n",
    "            \"Confidence\": \"Medium\",\n",
    "            \"Rationale\": \"Using weight oscillation dampener but not adaptive version. Adaptive version adjusts dampening strength automatically.\",\n",
    "            \"Statistical Evidence\": \"Best practice for oscillation dampening\",\n",
    "            \"Expected Impact\": \"Better automatic tuning of dampening strength\",\n",
    "            \"Priority\": \"Medium\"\n",
    "        })\n",
    "    \n",
    "    # 10. Loss weighting strategy recommendation\n",
    "    current_loss_strategy = model_config.get(\"loss_weighting_strategy\", \"none\")\n",
    "    \n",
    "    if current_loss_strategy == \"none\" and not config_history.empty:\n",
    "        recommendations.append({\n",
    "            \"Parameter\": \"loss_weighting_strategy\",\n",
    "            \"Current\": \"none\",\n",
    "            \"Suggested\": [\"physics_aware\", \"balanced\", \"adaptive\"],\n",
    "            \"Confidence\": \"Medium\",\n",
    "            \"Rationale\": \"Not using loss weighting. Consider physics-aware weighting for physics simulations or balanced/adaptive for general use.\",\n",
    "            \"Statistical Evidence\": \"Loss weighting can improve multi-target optimization\",\n",
    "            \"Expected Impact\": \"Better balance between multiple output targets\",\n",
    "            \"Priority\": \"Medium\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(recommendations) if recommendations else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations_df = generate_advanced_recommendations(\n",
    "    model_config, training_config, config_history, results_df, CONFIDENCE_LEVEL\n",
    ")\n",
    "\n",
    "if not recommendations_df.empty:\n",
    "    display(Markdown(\"### Data-Driven Hyperparameter Recommendations\"))\n",
    "    display(recommendations_df)\n",
    "    \n",
    "    # Priority summary\n",
    "    high_priority = recommendations_df[recommendations_df[\"Priority\"] == \"High\"]\n",
    "    \n",
    "    if not high_priority.empty:\n",
    "        display(Markdown(f\"\\\\n**{len(high_priority)} HIGH PRIORITY recommendations require immediate attention.**\"))\n",
    "        \n",
    "        print(\"\\\\nTop Action Items:\")\n",
    "        for idx, row in high_priority.iterrows():\n",
    "            print(f\"  {idx+1}. Adjust {row['Parameter']}: {row['Rationale']}\")\n",
    "else:\n",
    "    display(Markdown(\"✓ Current hyperparameters appear well-tuned. No immediate adjustments recommended.\"))\n",
    "    print(\"Consider fine-tuning with small perturbations to explore local optimum neighborhood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd20f6",
   "metadata": {},
   "source": [
    "### Hyperparameter Impact Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config_history.empty:\n",
    "    # Select hyperparameter columns and performance metrics\n",
    "    hyperparam_cols = [col for col in [\"learning_rate\", \"dropout_rate\", \"train_batch_size\", \"train_epochs\"] \n",
    "                       if col in config_history.columns]\n",
    "    \n",
    "    metric_cols = [col for col in [\"best_r2\", \"final_r2\", \"total_training_time\", \"best_epoch\"] \n",
    "                   if col in config_history.columns]\n",
    "    \n",
    "    if hyperparam_cols and metric_cols:\n",
    "        # Create correlation matrix\n",
    "        analysis_df = config_history[hyperparam_cols + metric_cols].copy()\n",
    "        \n",
    "        # Normalize total_training_time to be on similar scale (invert so higher is better)\n",
    "        if \"total_training_time\" in analysis_df.columns:\n",
    "            max_time = analysis_df[\"total_training_time\"].max()\n",
    "            analysis_df[\"efficiency_score\"] = 1 - (analysis_df[\"total_training_time\"] / max_time)\n",
    "            metric_cols.append(\"efficiency_score\")\n",
    "            metric_cols.remove(\"total_training_time\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = analysis_df[hyperparam_cols + metric_cols].corr()\n",
    "        \n",
    "        # Extract hyperparameter vs metric correlations\n",
    "        hp_metric_corr = corr_matrix.loc[hyperparam_cols, metric_cols]\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.heatmap(hp_metric_corr, annot=True, fmt=\".3f\", cmap=\"RdBu_r\", \n",
    "                    center=0, vmin=-1, vmax=1, \n",
    "                    cbar_kws={\"label\": \"Correlation Coefficient\"},\n",
    "                    linewidths=0.5, linecolor='gray', ax=ax)\n",
    "        \n",
    "        ax.set_title(\"Hyperparameter Impact on Performance Metrics\", fontsize=14, fontweight=\"bold\", pad=15)\n",
    "        ax.set_xlabel(\"Performance Metrics\", fontsize=11)\n",
    "        ax.set_ylabel(\"Hyperparameters\", fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        heatmap_path = paths[\"figures_dir\"] / \"hyperparameter_impact_heatmap.png\"\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n",
    "        display(Markdown(f\"✓ Saved hyperparameter impact heatmap to `{heatmap_path.name}`\"))\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\\\n**Heatmap Interpretation:**\")\n",
    "        print(\"• Strong positive correlation (red): Increasing hyperparameter improves metric\")\n",
    "        print(\"• Strong negative correlation (blue): Increasing hyperparameter decreases metric\")\n",
    "        print(\"• Near-zero correlation (white): Little to no linear relationship\")\n",
    "        \n",
    "        # Find strongest relationships\n",
    "        flat_corr = hp_metric_corr.abs().values.flatten()\n",
    "        flat_indices = np.argsort(flat_corr)[::-1]\n",
    "        \n",
    "        print(\"\\\\n**Strongest Relationships:**\")\n",
    "        for i in range(min(3, len(flat_indices))):\n",
    "            row_idx = flat_indices[i] // len(metric_cols)\n",
    "            col_idx = flat_indices[i] % len(metric_cols)\n",
    "            \n",
    "            hp = hp_metric_corr.index[row_idx]\n",
    "            metric = hp_metric_corr.columns[col_idx]\n",
    "            corr_value = hp_metric_corr.iloc[row_idx, col_idx]\n",
    "            \n",
    "            direction = \"positively\" if corr_value > 0 else \"negatively\"\n",
    "            print(f\"  {i+1}. {hp} correlates {direction} with {metric} (r={corr_value:.3f})\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠ Insufficient historical data for hyperparameter impact analysis\")\n",
    "else:\n",
    "    print(\"⚠ No configuration history available for hyperparameter impact analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447a7f",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d7d97",
   "metadata": {},
   "source": [
    "### Key Findings and Action Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "summary_findings = []\n",
    "\n",
    "# 1. Training performance\n",
    "best_val_loss = results_df[\"val_loss\"].min()\n",
    "final_val_loss = results_df.iloc[-1][\"val_loss\"]\n",
    "best_r2 = results_df[\"r2_score\"].max()\n",
    "\n",
    "summary_findings.append({\n",
    "    \"Category\": \"Training Performance\",\n",
    "    \"Finding\": f\"Best validation loss: {best_val_loss:.4f} at epoch {results_df['val_loss'].idxmin()}\",\n",
    "    \"Status\": \"✓\" if best_val_loss < 0.1 else \"⚠\",\n",
    "    \"Action\": \"Continue training if best epoch near end\" if results_df[\"val_loss\"].idxmin() > len(results_df) * 0.8 else \"No action needed\"\n",
    "})\n",
    "\n",
    "summary_findings.append({\n",
    "    \"Category\": \"Model Accuracy\",\n",
    "    \"Finding\": f\"Peak R² score: {best_r2:.4f}\",\n",
    "    \"Status\": \"✓\" if best_r2 > 0.8 else \"⚠\" if best_r2 > 0.6 else \"✗\",\n",
    "    \"Action\": \"Excellent performance\" if best_r2 > 0.9 else \"Consider architecture changes\" if best_r2 < 0.7 else \"Minor tuning recommended\"\n",
    "})\n",
    "\n",
    "# 2. Generalization\n",
    "final_gap = results_df.iloc[-1][\"train_val_gap\"]\n",
    "\n",
    "summary_findings.append({\n",
    "    \"Category\": \"Generalization\",\n",
    "    \"Finding\": f\"Train/val gap: {final_gap:.4f}\",\n",
    "    \"Status\": \"✓\" if final_gap < 0.05 else \"⚠\" if final_gap < 0.1 else \"✗\",\n",
    "    \"Action\": \"Good generalization\" if final_gap < 0.05 else \"Increase regularization (dropout)\" if final_gap > 0.1 else \"Monitor closely\"\n",
    "})\n",
    "\n",
    "# 3. Convergence stability\n",
    "loss_std = results_df.tail(10)[\"val_loss\"].std()\n",
    "\n",
    "summary_findings.append({\n",
    "    \"Category\": \"Convergence Stability\",\n",
    "    \"Finding\": f\"Loss std (last 10 epochs): {loss_std:.4f}\",\n",
    "    \"Status\": \"✓\" if loss_std < 0.01 else \"⚠\",\n",
    "    \"Action\": \"Stable convergence\" if loss_std < 0.01 else \"Consider learning rate schedule\"\n",
    "})\n",
    "\n",
    "# 4. Prediction quality (only if model loaded)\n",
    "if residual_metrics and model is not None:\n",
    "    mae = residual_metrics.get(\"mae\", float('inf'))\n",
    "    \n",
    "    summary_findings.append({\n",
    "        \"Category\": \"Prediction Quality\",\n",
    "        \"Finding\": f\"Mean absolute error: {mae:.4f}\",\n",
    "        \"Status\": \"✓\" if mae < 0.5 else \"⚠\" if mae < 1.0 else \"✗\",\n",
    "        \"Action\": \"Excellent predictions\" if mae < 0.5 else \"Review feature engineering\" if mae > 1.0 else \"Minor improvements possible\"\n",
    "    })\n",
    "else:\n",
    "    summary_findings.append({\n",
    "        \"Category\": \"Prediction Quality\",\n",
    "        \"Finding\": \"Model checkpoint not loaded\",\n",
    "        \"Status\": \"⚠\",\n",
    "        \"Action\": \"Run new training with current config to enable prediction analysis\"\n",
    "    })\n",
    "\n",
    "# 5. Computational efficiency\n",
    "avg_epoch_time = results_df.tail(10)[\"epoch_time\"].mean()\n",
    "total_time = results_df[\"epoch_time\"].sum()\n",
    "\n",
    "summary_findings.append({\n",
    "    \"Category\": \"Computational Efficiency\",\n",
    "    \"Finding\": f\"Avg epoch time: {avg_epoch_time:.2f}s | Total: {total_time:.1f}s\",\n",
    "    \"Status\": \"✓\" if avg_epoch_time < 5.0 else \"⚠\",\n",
    "    \"Action\": \"Efficient training\" if avg_epoch_time < 5.0 else \"Consider batch size optimization\"\n",
    "})\n",
    "\n",
    "# 6. Weight constraint effectiveness (if James-Stein comparison was run)\n",
    "if 'comparison_results' in locals() and comparison_results:\n",
    "    bc_best = comparison_results[\"Binary Constraints\"][\"best_val_loss\"]\n",
    "    js_best = comparison_results[\"James-Stein\"][\"best_val_loss\"]\n",
    "    nc_best = comparison_results[\"No Constraints\"][\"best_val_loss\"]\n",
    "    \n",
    "    if bc_best < min(js_best, nc_best):\n",
    "        constraint_status = \"✓\"\n",
    "        constraint_action = \"Binary constraints are most effective\"\n",
    "    elif js_best < min(bc_best, nc_best):\n",
    "        constraint_status = \"⚠\"\n",
    "        constraint_action = \"Consider switching to James-Stein shrinkage\"\n",
    "    else:\n",
    "        constraint_status = \"⚠\"\n",
    "        constraint_action = \"Constraints may not be beneficial for this problem\"\n",
    "    \n",
    "    summary_findings.append({\n",
    "        \"Category\": \"Weight Constraints\",\n",
    "        \"Finding\": f\"Best: Binary={bc_best:.4f}, JS={js_best:.4f}, None={nc_best:.4f}\",\n",
    "        \"Status\": constraint_status,\n",
    "        \"Action\": constraint_action\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_findings)\n",
    "\n",
    "display(Markdown(\"### Executive Summary\"))\n",
    "display(summary_df)\n",
    "\n",
    "# Count status indicators\n",
    "success_count = (summary_df[\"Status\"] == \"✓\").sum()\n",
    "warning_count = (summary_df[\"Status\"] == \"⚠\").sum()\n",
    "critical_count = (summary_df[\"Status\"] == \"✗\").sum()\n",
    "\n",
    "print(f\"\\\\nOverall Assessment:\")\n",
    "print(f\"  ✓ Success indicators: {success_count}\")\n",
    "print(f\"  ⚠ Warning indicators: {warning_count}\")\n",
    "print(f\"  ✗ Critical issues: {critical_count}\")\n",
    "\n",
    "if critical_count > 0:\n",
    "    print(\"\\\\n⚠️ CRITICAL ISSUES DETECTED - Immediate attention required\")\n",
    "elif warning_count > 2:\n",
    "    print(\"\\\\n⚠ Multiple warnings detected - Review recommendations carefully\")\n",
    "else:\n",
    "    print(\"\\\\n✓ Training appears healthy overall\")\n",
    "\n",
    "# Prioritized action items\n",
    "display(Markdown(\"### Prioritized Action Items\"))\n",
    "\n",
    "action_items = []\n",
    "\n",
    "# From recommendations\n",
    "if 'recommendations_df' in locals() and not recommendations_df.empty:\n",
    "    high_priority_recs = recommendations_df[recommendations_df[\"Priority\"] == \"High\"]\n",
    "    \n",
    "    for idx, row in high_priority_recs.iterrows():\n",
    "        action_items.append({\n",
    "            \"Priority\": 1,\n",
    "            \"Action\": f\"Adjust {row['Parameter']}: {row['Suggested'][0] if isinstance(row['Suggested'], list) else row['Suggested']}\",\n",
    "            \"Rationale\": row['Rationale'],\n",
    "            \"Expected Impact\": row['Expected Impact']\n",
    "        })\n",
    "\n",
    "# From summary findings\n",
    "for idx, row in summary_df.iterrows():\n",
    "    if row[\"Status\"] == \"✗\":\n",
    "        action_items.append({\n",
    "            \"Priority\": 1,\n",
    "            \"Action\": row['Action'],\n",
    "            \"Rationale\": row['Finding'],\n",
    "            \"Expected Impact\": \"Critical improvement needed\"\n",
    "        })\n",
    "    elif row[\"Status\"] == \"⚠\" and row[\"Action\"] not in [\"Monitor closely\", \"Minor tuning recommended\"]:\n",
    "        action_items.append({\n",
    "            \"Priority\": 2,\n",
    "            \"Action\": row['Action'],\n",
    "            \"Rationale\": row['Finding'],\n",
    "            \"Expected Impact\": \"Performance optimization\"\n",
    "        })\n",
    "\n",
    "# Sort by priority\n",
    "action_items_df = pd.DataFrame(action_items)\n",
    "\n",
    "if not action_items_df.empty:\n",
    "    action_items_df = action_items_df.sort_values(\"Priority\").reset_index(drop=True)\n",
    "    action_items_df.index = action_items_df.index + 1  # Start from 1\n",
    "    display(action_items_df)\n",
    "else:\n",
    "    print(\"✓ No critical action items. System is performing well.\")\n",
    "    print(\"\\\\nOptional improvements:\")\n",
    "    print(\"  • Fine-tune hyperparameters with small perturbations\")\n",
    "    print(\"  • Explore ensemble methods for marginal gains\")\n",
    "    print(\"  • Consider transfer learning if applicable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa90823",
   "metadata": {},
   "source": [
    "### Analysis Artifacts and Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated figures\n",
    "figures_dir = paths[\"figures_dir\"]\n",
    "generated_figures = list(figures_dir.glob(\"*.png\"))\n",
    "\n",
    "display(Markdown(f\"### Generated Visualizations ({len(generated_figures)} files)\"))\n",
    "\n",
    "figure_summary = []\n",
    "\n",
    "for fig_path in sorted(generated_figures):\n",
    "    size_kb = fig_path.stat().st_size / 1024\n",
    "    \n",
    "    figure_summary.append({\n",
    "        \"Filename\": fig_path.name,\n",
    "        \"Size (KB)\": f\"{size_kb:.1f}\",\n",
    "        \"Modified\": pd.Timestamp(fig_path.stat().st_mtime, unit=\"s\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    })\n",
    "\n",
    "figure_summary_df = pd.DataFrame(figure_summary)\n",
    "\n",
    "if not figure_summary_df.empty:\n",
    "    display(figure_summary_df)\n",
    "    print(f\"\\\\nAll figures saved to: {figures_dir}\")\n",
    "else:\n",
    "    print(\"No figures generated in this session\")\n",
    "\n",
    "# Export summary statistics to JSON\n",
    "summary_export = {\n",
    "    \"analysis_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    \"model_checkpoint_epoch\": checkpoint_meta[\"epoch\"] if checkpoint_meta else None,\n",
    "    \"best_validation_loss\": float(results_df[\"val_loss\"].min()),\n",
    "    \"best_r2_score\": float(results_df[\"r2_score\"].max()),\n",
    "    \"final_train_val_gap\": float(results_df.iloc[-1][\"train_val_gap\"]),\n",
    "    \"total_training_epochs\": int(results_df.iloc[-1][\"epoch\"]),\n",
    "    \"total_training_time_seconds\": float(results_df[\"epoch_time\"].sum()),\n",
    "    \"average_epoch_time_seconds\": float(results_df[\"epoch_time\"].mean()),\n",
    "    \"prediction_mae\": float(residual_metrics.get(\"mae\", 0)) if residual_metrics else None,\n",
    "    \"prediction_rmse\": float(residual_metrics.get(\"rmse\", 0)) if residual_metrics else None,\n",
    "    \"artifacts_generated\": len(generated_figures),\n",
    "    \"recommendations_count\": len(recommendations_df) if 'recommendations_df' in locals() else 0,\n",
    "    \"model_config\": {\n",
    "        \"hidden_layers\": model_config.get(\"hidden_layers\"),\n",
    "        \"activation\": model_config.get(\"activation\"),\n",
    "        \"learning_rate\": model_config.get(\"learning_rate\"),\n",
    "        \"dropout_rate\": model_config.get(\"dropout_rate\"),\n",
    "        \"use_batch_norm\": model_config.get(\"use_batch_norm\"),\n",
    "        \"batch_norm_momentum\": model_config.get(\"batch_norm_momentum\"),\n",
    "        \"l2_regularization\": model_config.get(\"l2_regularization\"),\n",
    "        \"enable_binary_change_max\": model_config.get(\"enable_binary_change_max\"),\n",
    "        \"max_additional_binary_digits\": model_config.get(\"max_additional_binary_digits\"),\n",
    "        \"enable_weight_oscillation_dampener\": model_config.get(\"enable_weight_oscillation_dampener\"),\n",
    "        \"use_adaptive_oscillation_dampener\": model_config.get(\"use_adaptive_oscillation_dampener\"),\n",
    "        \"loss_weighting_strategy\": model_config.get(\"loss_weighting_strategy\"),\n",
    "        \"constraint_interval\": model_config.get(\"constraint_interval\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add James-Stein comparison results if available\n",
    "if 'comparison_results' in locals() and comparison_results:\n",
    "    summary_export[\"james_stein_comparison\"] = {\n",
    "        \"binary_constraints_best_loss\": float(comparison_results[\"Binary Constraints\"][\"best_val_loss\"]),\n",
    "        \"james_stein_best_loss\": float(comparison_results[\"James-Stein\"][\"best_val_loss\"]),\n",
    "        \"no_constraints_best_loss\": float(comparison_results[\"No Constraints\"][\"best_val_loss\"]),\n",
    "        \"winner\": min(comparison_results.items(), key=lambda x: x[1][\"best_val_loss\"])[0]\n",
    "    }\n",
    "\n",
    "summary_json_path = paths[\"analysis_dir\"] / f\"analysis_summary_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(summary_json_path, 'w') as f:\n",
    "    json.dump(summary_export, f, indent=2)\n",
    "\n",
    "display(Markdown(f\"\\\\n✓ Exported analysis summary to `{summary_json_path.name}`\"))\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\\\nSession timestamp: {summary_export['analysis_timestamp']}\")\n",
    "print(f\"Figures directory: {figures_dir}\")\n",
    "print(f\"Summary JSON: {summary_json_path}\")\n",
    "print(\"\\\\nNext steps:\")\n",
    "print(\"  1. Review high-priority recommendations\")\n",
    "print(\"  2. Implement suggested hyperparameter changes\")\n",
    "print(\"  3. Re-run training with updated configuration\")\n",
    "print(\"  4. Re-execute this notebook to track improvements\")\n",
    "print(\"\\\\nFor questions or issues, refer to the README.md documentation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
